(window.webpackJsonp=window.webpackJsonp||[]).push([[14],{337:function(e,t,i){"use strict";i.r(t),t.default=i.p+"assets/images/orientation-datasets-15615fe8643c50cd5ad61e4b00fa8250.svg"},338:function(e,t,i){"use strict";i.r(t),t.default=i.p+"assets/images/orientation-sample-75b7b005f45944ade1b10a6c2199641d.svg"},339:function(e,t,i){"use strict";i.r(t),t.default=i.p+"assets/images/orientation-prediction-e0d5d34f70eec00b2d293c49a7adf0de.svg"},340:function(e,t,i){"use strict";i.r(t),t.default=i.p+"assets/images/orientation-prediction-1-4a5b71d4f5648358e8d004e5cf4cccd4.svg"},341:function(e,t,i){"use strict";i.r(t),t.default=i.p+"assets/images/orientation-prediction-2-db6b70e4daeb1ca70ec55509ea75504c.svg"},70:function(e,t,i){"use strict";i.r(t),i.d(t,"frontMatter",(function(){return s})),i.d(t,"metadata",(function(){return c})),i.d(t,"rightToc",(function(){return d})),i.d(t,"default",(function(){return m}));var n=i(2),a=i(6),o=(i(0),i(73)),r=i(83),l=i(80),s={id:"orientation",sidebar_label:"2. Orientation",title:"Orientation Module",slug:"/orientation-module"},c={unversionedId:"orientation",id:"orientation",isDocsHomePage:!1,title:"Orientation Module",description:"To predict the orientation of an aligned student-id image inputted from the detection module, we shall quickly develop an image classification model and train it on our orientation dataset. We expect the trained orientation model to predict the confidence scores for orientation angles (90, 180, 270, and 360) for an input image.",source:"@site/docs/orientation.md",slug:"/orientation-module",permalink:"/pytorch-for-information-extraction/orientation-module",editUrl:"https://github.com/MbassiJaphet/pytorch-for-information-extraction/edit/master/docs/orientation.md",version:"current",sidebar_label:"2. Orientation",sidebar:"tutorial",previous:{title:"Detection Module",permalink:"/pytorch-for-information-extraction/detection-module"},next:{title:"Extraction Module",permalink:"/pytorch-for-information-extraction/extraction-module"}},d=[{value:"<strong>2.1. Orientation Dataset</strong>",id:"21-orientation-dataset",children:[{value:"2.1.1. Define transforms for orientation datasets",id:"211-define-transforms-for-orientation-datasets",children:[]},{value:"2.1.2.  Instantiate orientation datasets",id:"212--instantiate-orientation-datasets",children:[]},{value:"2.1.3. Visualize orientation dataset",id:"213-visualize-orientation-dataset",children:[]}]},{value:"<strong>2.2. Orientation Model</strong>",id:"22-orientation-model",children:[{value:"2.2.1. Define Orientation Model",id:"221-define-orientation-model",children:[]},{value:"2.2.2. Specify checkpoint and instantiate the model",id:"222-specify-checkpoint-and-instantiate-the-model",children:[]}]},{value:"<strong>2.3. Training and Validation</strong>",id:"23-training-and-validation",children:[{value:"2.3.1. Specify data loaders",id:"231-specify-data-loaders",children:[]},{value:"2.3.2. Define loss function and optimizer",id:"232-define-loss-function-and-optimizer",children:[]},{value:"2.3.3. Define training function",id:"233-define-training-function",children:[]},{value:"2.3.4. Train orientation model",id:"234-train-orientation-model",children:[]},{value:"2.3.5. Resume training orientation model",id:"235-resume-training-orientation-model",children:[]},{value:"2.3.6. Evaluate orientation model",id:"236-evaluate-orientation-model",children:[]}]},{value:"2.4 Orientation Correction",id:"24-orientation-correction",children:[]}],b={rightToc:d};function m(e){var t=e.components,s=Object(a.a)(e,["components"]);return Object(o.b)("wrapper",Object(n.a)({},b,s,{components:t,mdxType:"MDXLayout"}),Object(o.b)("p",null,"To predict the orientation of an aligned student-id image inputted from the detection module, we shall quickly develop an image classification model and train it on our ",Object(o.b)("a",Object(n.a)({parentName:"p"},{href:"https://github.com/MbassiJaphet/pytorch-for-information-extraction/tree/master/code/datasets/orientation"}),"orientation dataset"),". We expect the trained orientation model to predict the confidence scores for orientation angles (90, 180, 270, and 360) for an input image."),Object(o.b)("p",null,"So, let's resolve the imports of our orientation module."),Object(o.b)(r.a,{lines:[5,6,17,19],file:"orientation_module_imports",mdxType:"CodeBlock"}),Object(o.b)("h2",{id:"21-orientation-dataset"},Object(o.b)("strong",{parentName:"h2"},"2.1. Orientation Dataset")),Object(o.b)("p",null,"The ",Object(o.b)("a",Object(n.a)({parentName:"p"},{href:"https://github.com/MbassiJaphet/pytorch-for-information-extraction/tree/master/code/datasets/orientation"}),"orientation dataset")," consist of folders containing four subfolders, whereby each subfolder is named according to one of the four orientation classes i.e. ",Object(o.b)("strong",{parentName:"p"},"'090'"),", ",Object(o.b)("strong",{parentName:"p"},"'180'"),", ",Object(o.b)("strong",{parentName:"p"},"'270'"),", and ",Object(o.b)("strong",{parentName:"p"},"'360'"),". Each subfolder contains images rotated according to their folder's name."),Object(o.b)("p",null,"Pytorch provides ",Object(o.b)("a",Object(n.a)({parentName:"p"},{href:"https://pytorch.org/docs/stable/torchvision/datasets.html#torchvision.datasets.ImageFolder"}),"torchvision.datasets.ImageFolder")," for loading datasets with such format without requiring us to hardcode a custom dataset class for the data like we did for the detection dataset.\n",Object(o.b)("img",{alt:"img",src:i(337).default})),Object(o.b)("h3",{id:"211-define-transforms-for-orientation-datasets"},"2.1.1. Define transforms for orientation datasets"),Object(o.b)("p",null,"Before instantiating our various orientation datasets, we have to define the various transforms which shall be used to initialize them."),Object(o.b)(r.a,{lines:[1,4,5,6,7,8,9,10,14,21],file:"orientation_dataset_transforms",mdxType:"CodeBlock"}),Object(o.b)("details",{open:!0},Object(o.b)("summary",null,"Code Brieffings"),Object(o.b)("ul",null,Object(o.b)("li",{parentName:"ul"},"Defined and initialized various transforms specific to each of our orientation datasets(training, validation, and testing). We did that by ",Object(o.b)("strong",{parentName:"li"},"importing")," and leveraging ",Object(o.b)("a",Object(n.a)({parentName:"li"},{href:"https://pytorch.org/docs/stable/torchvision/transforms.html?highlight=torchvision%20transforms"}),"torchvision.transforms")," which is a module containing common image transformations."),Object(o.b)("li",{parentName:"ul"},Object(o.b)("a",Object(n.a)({parentName:"li"},{href:"https://pytorch.org/docs/stable/torchvision/transforms.html?highlight=transforms%20compose#torchvision.transforms.Compose"}),"transforms.Compose")," composes several transforms together."),Object(o.b)("li",{parentName:"ul"},Object(o.b)("a",Object(n.a)({parentName:"li"},{href:"https://pytorch.org/docs/stable/torchvision/transforms.html?highlight=transforms%20resize#torchvision.transforms.Resize"}),"transforms.Resize")," resizes the input image to the given size."),Object(o.b)("li",{parentName:"ul"},Object(o.b)("a",Object(n.a)({parentName:"li"},{href:"https://pytorch.org/docs/stable/torchvision/transforms.html?highlight=transforms%20randomaffine#torchvision.transforms.RandomAffine"}),"transforms.RandomAffine")," randomly affines transformation of the image keeping center invariant."),Object(o.b)("li",{parentName:"ul"},Object(o.b)("a",Object(n.a)({parentName:"li"},{href:"https://pytorch.org/docs/stable/torchvision/transforms.html?highlight=transforms%20randomapply#torchvision.transforms.RandomApply"}),"transforms.RandomApply")," randomly a list of transformations with a given probability."),Object(o.b)("li",{parentName:"ul"},Object(o.b)("a",Object(n.a)({parentName:"li"},{href:"https://pytorch.org/docs/stable/torchvision/transforms.html?highlight=transforms%20randomgrayscale#torchvision.transforms.RandomGrayscale"}),"transforms.RandomGrayscale")," randomly convert image to grayscale with a probability of ",Object(o.b)("strong",{parentName:"li"},"'p'"),"."),Object(o.b)("li",{parentName:"ul"},Object(o.b)("a",Object(n.a)({parentName:"li"},{href:"https://pytorch.org/docs/stable/torchvision/transforms.html?highlight=transforms%20normalize#torchvision.transforms.Normalize"}),"transforms.Normalize")," normalize a tensor image with mean and standard deviation."),Object(o.b)("li",{parentName:"ul"},Object(o.b)("strong",{parentName:"li"},"Note")," that we only applied data augmentation on the training dataset. This is so that our model can easily generalize input data."))),Object(o.b)("h3",{id:"212--instantiate-orientation-datasets"},"2.1.2.  Instantiate orientation datasets"),Object(o.b)("p",null,"We shall leverage Pytorch inbuilt torchvision.datasets.ImageFolder class to effortlessly instantiate our orientation training, validation, and testing datasets."),Object(o.b)(r.a,{lines:[1,10,12,14],file:"orientation_dataset_init",mdxType:"CodeBlock"}),Object(o.b)("details",{open:!0},Object(o.b)("summary",null,"Code Brieffings"),Object(o.b)("ul",null,Object(o.b)("li",{parentName:"ul"},"We initialized ",Object(o.b)("strong",{parentName:"li"},"training"),", ",Object(o.b)("strong",{parentName:"li"},"validation"),", and ",Object(o.b)("strong",{parentName:"li"},"testing")," datasets using ",Object(o.b)("a",Object(n.a)({parentName:"li"},{href:"https://pytorch.org/docs/stable/torchvision/datasets.html#torchvision.datasets.ImageFolder"}),"torchvision.datasets.ImageFolder")," with their respective ",Object(o.b)("strong",{parentName:"li"},"folders"),"."),Object(o.b)("li",{parentName:"ul"},"We initialized the variables",Object(o.b)("inlineCode",{parentName:"li"},"orientation_classes"),", and ",Object(o.b)("inlineCode",{parentName:"li"},"num_orientation_classes")," to values of our ",Object(o.b)("strong",{parentName:"li"},"orientation classes")," and their ",Object(o.b)("strong",{parentName:"li"},"number")," respectively."))),Object(o.b)("p",null),Object(o.b)("p",null,"Just checking the names and number of classes from our orientation dataset to make sure everything is ",Object(o.b)("strong",{parentName:"p"},"OK"),"!"),Object(o.b)(r.a,{lines:[7,8,10,11],file:"orientation_dataset_classes",mdxType:"CodeBlock"}),Object(o.b)(l.a,{file:"orientation_dataset_classes_output",mdxType:"OutputBlock"}),Object(o.b)("h3",{id:"213-visualize-orientation-dataset"},"2.1.3. Visualize orientation dataset"),Object(o.b)(r.a,{lines:[1,7,8,10,11],file:"orientation_dataset_visualize",mdxType:"CodeBlock"}),Object(o.b)("details",{open:!0},Object(o.b)("summary",null,"Code Brieffings"),Object(o.b)("ul",null,Object(o.b)("li",{parentName:"ul"},"Randomly selected a group of four elements from our orientation training dataset as ",Object(o.b)("inlineCode",{parentName:"li"},"(image_tensor, label_tensor)")," pairs."),Object(o.b)("li",{parentName:"ul"},"Denormalized ",Object(o.b)("inlineCode",{parentName:"li"},"image_tensor")," for each pair and had each image plotted displaying their corresponding classes."))),Object(o.b)("p",null,Object(o.b)("img",{alt:"img",src:i(338).default})),Object(o.b)("h2",{id:"22-orientation-model"},Object(o.b)("strong",{parentName:"h2"},"2.2. Orientation Model")),Object(o.b)("h3",{id:"221-define-orientation-model"},"2.2.1. Define Orientation Model"),Object(o.b)("p",null,Object(o.b)("strong",{parentName:"p"},"Note")," that the model architecture defined below expects input image tensors of shape  ",Object(o.b)("strong",{parentName:"p"},"(3 x 224 x 224)")," taking after transforms of the orientation datasets.\nLet's define an architecture for our orientation model from scratch."),Object(o.b)(r.a,{lines:[1,2,4,8,9,10,13,14,17,18,20,22,24,25,26,28,30,31,32],file:"orientation_model",mdxType:"CodeBlock"}),Object(o.b)("details",{open:!0},Object(o.b)("summary",null,"Code Brieffings"),Object(o.b)("ul",null,Object(o.b)("li",{parentName:"ul"},"Defined ",Object(o.b)("inlineCode",{parentName:"li"},"OrientationModel")," extending ",Object(o.b)("a",Object(n.a)({parentName:"li"},{href:"https://pytorch.org/docs/stable/generated/torch.nn.Module.html?highlight=torch%20nn%20module#torch.nn.Module"}),"torch.nn.Module"),". The constructor argument ",Object(o.b)("inlineCode",{parentName:"li"},"num_classes")," is equivalent to desired number of ",Object(o.b)("strong",{parentName:"li"},"classes/labels"),"."),Object(o.b)("li",{parentName:"ul"},"Defined the feed-forward behavior of the neural network by overriding the ",Object(o.b)("a",Object(n.a)({parentName:"li"},{href:"https://pytorch.org/docs/stable/generated/torch.nn.Module.html?highlight=torch%20nn%20module%20forward#torch.nn.Module.forward"}),Object(o.b)("inlineCode",{parentName:"a"},"forward"))," method."))),Object(o.b)("p",null,"Now that we have defined the architecture of our orientation model, let's define the helper function to instantiate it !"),Object(o.b)(r.a,{lines:[4],file:"orientation_model_init_function",mdxType:"CodeBlock"}),Object(o.b)("details",{open:!0},Object(o.b)("summary",null,"Code Brieffings"),Object(o.b)("ul",null,Object(o.b)("li",{parentName:"ul"},"Used ",Object(o.b)("a",Object(n.a)({parentName:"li"},{href:"https://pytorch.org/docs/stable/generated/torch.nn.Module.html?highlight=load_state_dict#torch.nn.Module.load_state_dict"}),"orientation_model.load_state_dict()")," to set model weights from state dictionary if ",Object(o.b)("inlineCode",{parentName:"li"},"state_dict")," is given."))),Object(o.b)("h3",{id:"222-specify-checkpoint-and-instantiate-the-model"},"2.2.2. Specify checkpoint and instantiate the model"),Object(o.b)("p",null,"Looking forward to ",Object(o.b)("strong",{parentName:"p"},"resumable")," training and saving of our orientation model, we shall now specify the checkpoints for the ",Object(o.b)("strong",{parentName:"p"},"state dictionaries")," of the model and its training optimizer while initializing the model at once."),Object(o.b)(r.a,{lines:[2,14,22],file:"orientation_checkpoint",mdxType:"CodeBlock"}),Object(o.b)("details",{open:!0},Object(o.b)("summary",null,"Code Brieffings"),Object(o.b)("ul",null,Object(o.b)("li",{parentName:"ul"},"Selected available computational hardware using ",Object(o.b)("a",Object(n.a)({parentName:"li"},{href:"https://pytorch.org/docs/stable/tensor_attributes.html?highlight=torch%20device#torch.torch.device"}),"torch.device()"),"."))),Object(o.b)(l.a,{file:"orientation_checkpoint_output",mdxType:"OutputBlock"}),Object(o.b)("p",null,"Now let's print our orientation model to check if it has been initialized as we expect."),Object(o.b)(r.a,{file:"orientation_model_visualize",mdxType:"CodeBlock"}),Object(o.b)(l.a,{file:"orientation_model_visualize_output",mdxType:"OutputBlock"}),Object(o.b)("h2",{id:"23-training-and-validation"},Object(o.b)("strong",{parentName:"h2"},"2.3. Training and Validation")),Object(o.b)("h3",{id:"231-specify-data-loaders"},"2.3.1. Specify data loaders"),Object(o.b)("p",null,"After initializing the various orientation datasets, let us use them to specify data loaders which shall be used for training, validation, and testing."),Object(o.b)(r.a,{lines:[4,8,12,17],file:"orientation_dataset_loaders",mdxType:"CodeBlock"}),Object(o.b)("details",{open:!0},Object(o.b)("summary",null,"Code Brieffings"),Object(o.b)("ul",null,Object(o.b)("li",{parentName:"ul"},"Initialized data loaders for each of our orientation datasets (training, validation and testing) by using ",Object(o.b)("a",Object(n.a)({parentName:"li"},{href:"https://pytorch.org/docs/stable/data.html?highlight=torch%20utils%20data%20dataloader#torch.utils.data.DataLoader"}),"torch.utils.data.DataLoader()"),"."),Object(o.b)("li",{parentName:"ul"},"Initialized the dictionary variable '",Object(o.b)("inlineCode",{parentName:"li"},"orientation_loaders "),"', which references all of the data loaders."))),Object(o.b)("h3",{id:"232-define-loss-function-and-optimizer"},"2.3.2. Define loss function and optimizer"),Object(o.b)("p",null,"Let's initialize the optimizer for training the orientation model, and get ready for training !"),Object(o.b)(r.a,{lines:[2,4,6,11],file:"orientation_optimizer",mdxType:"CodeBlock"}),Object(o.b)(l.a,{file:"orientation_optimizer_init_output",mdxType:"OutputBlock"}),Object(o.b)("h3",{id:"233-define-training-function"},"2.3.3. Define training function"),Object(o.b)(r.a,{lines:[4,21,35,36,38,41,42,43,44,45,47,49,53,54,58,59,61],file:"orientation_model_train_function",mdxType:"CodeBlock"}),Object(o.b)("details",{open:!0},Object(o.b)("summary",null,"Code Brieffings"),Object(o.b)("ul",null,Object(o.b)("li",{parentName:"ul"},"Moved the ",Object(o.b)("strong",{parentName:"li"},"model")," to the computation device as the ",Object(o.b)("inlineCode",{parentName:"li"},"(data, target)")," pairs from ",Object(o.b)("inlineCode",{parentName:"li"},"loaders['train']"),"."),Object(o.b)("li",{parentName:"ul"},"Within the training loop, we reset the gradients before predicting ",Object(o.b)("inlineCode",{parentName:"li"},"output")," for each ",Object(o.b)("inlineCode",{parentName:"li"},"data"),", and its compute ",Object(o.b)("inlineCode",{parentName:"li"},"loss")," to its ",Object(o.b)("inlineCode",{parentName:"li"},"target"),"."),Object(o.b)("li",{parentName:"ul"},"Find best ",Object(o.b)("inlineCode",{parentName:"li"},"loss")," as ",Object(o.b)("inlineCode",{parentName:"li"},"valid_loss")," and update checkpoint accordingly."))),Object(o.b)("h3",{id:"234-train-orientation-model"},"2.3.4. Train orientation model"),Object(o.b)("p",null,"Now let's train our orientation model for 20 epochs."),Object(o.b)(r.a,{file:"orientation_model_train",mdxType:"CodeBlock"}),Object(o.b)(l.a,{file:"orientation_model_train_output",mdxType:"OutputBlock"}),Object(o.b)("h3",{id:"235-resume-training-orientation-model"},"2.3.5. Resume training orientation model"),Object(o.b)("p",null,"At the end of every epoch, we had the checkpoints of the orientation module updated. Now let's use these updated checkpoints to reload the orientation model with orientation optimizer and resume the training up to ",Object(o.b)("strong",{parentName:"p"},"'30'")," epochs."),Object(o.b)("div",{className:"admonition admonition-important alert alert--info"},Object(o.b)("div",Object(n.a)({parentName:"div"},{className:"admonition-heading"}),Object(o.b)("h5",{parentName:"div"},Object(o.b)("span",Object(n.a)({parentName:"h5"},{className:"admonition-icon"}),Object(o.b)("svg",Object(n.a)({parentName:"span"},{xmlns:"http://www.w3.org/2000/svg",width:"14",height:"16",viewBox:"0 0 14 16"}),Object(o.b)("path",Object(n.a)({parentName:"svg"},{fillRule:"evenodd",d:"M7 2.3c3.14 0 5.7 2.56 5.7 5.7s-2.56 5.7-5.7 5.7A5.71 5.71 0 0 1 1.3 8c0-3.14 2.56-5.7 5.7-5.7zM7 1C3.14 1 0 4.14 0 8s3.14 7 7 7 7-3.14 7-7-3.14-7-7-7zm1 3H6v5h2V4zm0 6H6v2h2v-2z"})))),"important")),Object(o.b)("div",Object(n.a)({parentName:"div"},{className:"admonition-content"}),Object(o.b)("p",{parentName:"div"},"To reload the orientation model and the orientation optimizer from the checkpoint, simply re-run the code cells in Section 2.2.2. and Section 2.3.2 respectively. Just make sure ",Object(o.b)("inlineCode",{parentName:"p"},"load_orientation_checkpoint")," is set to ",Object(o.b)("inlineCode",{parentName:"p"},"True"),". The resulting outputs shall be identical to the ones below."))),Object(o.b)("p",null,"Reloading orientation model from the checkpoint. (Section 2.2.2)"),Object(o.b)(r.a,{lines:[2,4,14,22],file:"orientation_checkpoint",mdxType:"CodeBlock"}),Object(o.b)("details",{open:!0},Object(o.b)("summary",null,"Code Brieffings"),Object(o.b)("ul",null,Object(o.b)("li",{parentName:"ul"},"Loaded checkpoint using ",Object(o.b)("a",Object(n.a)({parentName:"li"},{href:"https://pytorch.org/docs/stable/generated/torch.load.html?highlight=torch%20load#torch.load"}),"torch.load()"),". The argument ",Object(o.b)("inlineCode",{parentName:"li"},"map_location")," is used to specify the computing device into which the checkpoint is loaded."))),Object(o.b)(l.a,{file:"orientation_model_init_checkpoint_output",mdxType:"OutputBlock"}),Object(o.b)("p",null,"Reloading orientation optimizer from the checkpoint (Section 2.3.2)"),Object(o.b)(r.a,{lines:[2,4,6,8,9,11],file:"orientation_optimizer",mdxType:"CodeBlock"}),Object(o.b)("details",{open:!0},Object(o.b)("summary",null,"Code Brieffings"),Object(o.b)("ul",null,Object(o.b)("li",{parentName:"ul"},"Used ",Object(o.b)("a",Object(n.a)({parentName:"li"},{href:"https://pytorch.org/docs/stable/generated/torch.nn.Module.html?highlight=load_state_dict#torch.nn.Module.load_state_dict"}),"orientation_optimizer.load_state_dict()")," to initialize optimizer weights if ",Object(o.b)("inlineCode",{parentName:"li"},"orientation_optimizer_state_dict")," is available. This sets the optimizer to  the state after that of the previous training."))),Object(o.b)(l.a,{file:"orientation_optimizer_init_checkpoint_output",mdxType:"OutputBlock"}),Object(o.b)("p",null,"Now let's resume the training of our orientation model."),Object(o.b)(r.a,{file:"orientation_model_train_resume",mdxType:"CodeBlock"}),Object(o.b)(l.a,{file:"orientation_model_train_resume_output",mdxType:"OutputBlock"}),Object(o.b)("p",null,"You notice that the training starts from ",Object(o.b)("strong",{parentName:"p"},"epoch 21")," since the orientation model has already been trained for ",Object(o.b)("strong",{parentName:"p"},"20 epochs"),"."),Object(o.b)("h3",{id:"236-evaluate-orientation-model"},"2.3.6. Evaluate orientation model"),Object(o.b)("p",null,"To conclude on the performance of your models, it is always of good practice to evaluate them on sample data. We shall evaluate the performance of the orientation model on sample images from the testing dataset."),Object(o.b)("p",null,"But, before that let's define the test function."),Object(o.b)(r.a,{lines:[2,5,12,13,14,16,18,20,22,24,26,27],file:"orientation_model_test_function",mdxType:"CodeBlock"}),Object(o.b)("details",{open:!0},Object(o.b)("summary",null,"Code Brieffings"),Object(o.b)("ul",null,Object(o.b)("li",{parentName:"ul"},"Put the model to evaluation mode using ",Object(o.b)("a",Object(n.a)({parentName:"li"},{href:"https://pytorch.org/docs/stable/generated/torch.nn.Module.html?highlight=torch%20nn%20module%20eval#torch.nn.Module.eval"}),"model.eval()"),". This disables some training behaviors of our model such as ",Object(o.b)("strong",{parentName:"li"},"batch normalization")," and dropout layers."),Object(o.b)("li",{parentName:"ul"},"Iterate ",Object(o.b)("strong",{parentName:"li"},"batches")," of the ",Object(o.b)("strong",{parentName:"li"},"orientation test loader")," for ",Object(o.b)("inlineCode",{parentName:"li"},"(data, target)")," pairs."),Object(o.b)("li",{parentName:"ul"},"Move ",Object(o.b)("inlineCode",{parentName:"li"},"(data, target)")," pairs to computation ",Object(o.b)("strong",{parentName:"li"},"device/hardware")," using ",Object(o.b)("a",Object(n.a)({parentName:"li"},{href:"https://pytorch.org/docs/stable/tensors.html?highlight=torch%20tensor#torch.Tensor.to"}),"to()")," method."),Object(o.b)("li",{parentName:"ul"},"Predict ",Object(o.b)("inlineCode",{parentName:"li"},"output")," for ",Object(o.b)("inlineCode",{parentName:"li"},"data")," and compute ",Object(o.b)("inlineCode",{parentName:"li"},"loss")," to targets. Then the average total loss is computed as",Object(o.b)("inlineCode",{parentName:"li"},"test_loss"),"."))),Object(o.b)("p",null,"With our test function defined, we shall now use it to evaluate the performance of the orientation model on the orientation test dataset."),Object(o.b)(r.a,{file:"orientation_model_test",mdxType:"CodeBlock"}),Object(o.b)(l.a,{file:"orientation_model_test_output",mdxType:"OutputBlock"}),Object(o.b)("h2",{id:"24-orientation-correction"},"2.4 Orientation Correction"),Object(o.b)("p",null,"Let's properly visualize the performance of our orientation model via inference on sample images from the test dataset one at a time."),Object(o.b)("p",null,"Keep in mind that the objective behind an orientation module is to detect the orientation of an aligned document image, and to rectify it where necessary. Therefore, after inferencing every single image, we have shall apply the proper transformation to the image to rectify its orientation if necessary."),Object(o.b)(r.a,{lines:[8,9,10,14,16],file:"orientation_model_prediction_visualize",mdxType:"CodeBlock"}),Object(o.b)("p",null,Object(o.b)("img",{alt:"img",src:i(339).default})),Object(o.b)("details",null,Object(o.b)("summary",null,"More Outputs"),Object(o.b)("p",null,Object(o.b)("img",{alt:"img",src:i(340).default})),Object(o.b)("p",null),Object(o.b)("p",null,Object(o.b)("img",{alt:"img",src:i(341).default}))))}m.isMDXComponent=!0},80:function(e,t,i){"use strict";var n=i(3),a=i(0),o=i.n(a),r=function(e){function t(t){var i;return(i=e.call(this,t)||this)._currentFile=null,i.state={outputString:""},i}Object(n.a)(t,e),t.getDerivedStateFromProps=function(e,t){return e.id!==t.prevFile?{outputString:"",prevFile:e.file}:null};var i=t.prototype;return i.componentDidMount=function(){this._loadAsyncData(this.props.file)},i.componentDidUpdate=function(e,t){this.state.outputString||this._loadAsyncData(this.props.file)},i.componentWillUnmount=function(){this._currentFile=null},i.render=function(){return o.a.createElement("pre",{className:"output-block"},o.a.createElement("code",null,this.state.outputString))},i._loadAsyncData=function(e){var t=this;this._currentFile=e,fetch("/pytorch-for-information-extraction/code-snippets/"+e+".txt").then((function(e){return e.text()})).then((function(i){e===t._currentFile&&t.setState({outputString:i})})).catch((function(e){console.log(e)}))},t}(o.a.Component);t.a=r},83:function(e,t,i){"use strict";var n,a=i(3),o=i(0),r=i.n(o),l=(i(77),i(343)),s=i(342),c=function(e){function t(t){var i;return(i=e.call(this,t)||this).state={codeString:""},i._currentFile=null,i}Object(a.a)(t,e),t.getDerivedStateFromProps=function(e,t){return e.id!==t.prevFile?{codeString:"",prevFile:e.file}:null};var i=t.prototype;return i.componentDidMount=function(){this._loadAsyncData(this.props.file)},i.componentDidUpdate=function(e,t){this.state.codeString||this._loadAsyncData(this.props.file)},i.componentWillUnmount=function(){this._currentFile=null},i._highlightLine=function(e){var t={display:"block"};return this.props.lines&&this.props.lines.includes(e)&&(t.backgroundColor="rgb(144, 202, 249, 0.15)"),{style:t}},i.render=function(){return r.a.createElement("div",{class:"code-block"},r.a.createElement(l.a,{language:"python",lineProps:this._highlightLine.bind(this),wrapLines:!0,lineNumberStyle:{color:"#80d6ff"},style:s.a,showLineNumbers:!0,customStyle:d,codeTagProps:{style:{color:"#e0e0e0"}}},this.state.codeString))},i._loadAsyncData=function(e){var t=this;this._currentFile=e,fetch("/pytorch-for-information-extraction/code-snippets/"+e+".py").then((function(e){return e.text()})).then((function(i){e===t._currentFile&&t.setState({codeString:i})})).catch((function(e){console.log(e)}))},t}(r.a.Component);t.a=c;var d=((n={borderRadius:0,overflow:"auto",maxHeight:"75vh",fontSize:"0.67em"}).borderRadius=8,n)}}]);