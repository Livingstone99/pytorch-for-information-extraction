<!doctype html>
<html lang="en">
<head>
<meta charset="UTF-8">
<meta name="viewport" content="width=device-width,initial-scale=1">
<meta name="generator" content="Docusaurus v2.0.0-alpha.66">
<title data-react-helmet="true">Orientation Module | Pytorch For Information Extraction</title><meta data-react-helmet="true" name="twitter:card" content="summary_large_image"><meta data-react-helmet="true" name="docusaurus_language" content="en"><meta data-react-helmet="true" name="docusaurus_version" content="current"><meta data-react-helmet="true" name="docusaurus_tag" content="docs-default-current"><meta data-react-helmet="true" property="og:title" content="Orientation Module | Pytorch For Information Extraction"><meta data-react-helmet="true" name="description" content="To predict the orientation of an aligned student-id image inputted from the detection module, we shall quickly develop an image classification model and train it on our orientation dataset. We expect the trained orientation model to predict the confidence scores for orientation angles (90, 180, 270, and 360) for an input image."><meta data-react-helmet="true" property="og:description" content="To predict the orientation of an aligned student-id image inputted from the detection module, we shall quickly develop an image classification model and train it on our orientation dataset. We expect the trained orientation model to predict the confidence scores for orientation angles (90, 180, 270, and 360) for an input image."><meta data-react-helmet="true" property="og:url" content="https://mbassijaphet.github.io/pytorch-for-information-extraction//pytorch-for-information-extraction/orientation-module"><link data-react-helmet="true" rel="shortcut icon" href="/pytorch-for-information-extraction/img/favicon.ico"><link data-react-helmet="true" rel="canonical" href="https://mbassijaphet.github.io/pytorch-for-information-extraction//pytorch-for-information-extraction/orientation-module"><link rel="stylesheet" href="/pytorch-for-information-extraction/styles.3891f3db.css">
<link rel="preload" href="/pytorch-for-information-extraction/styles.bca2cbc6.js" as="script">
<link rel="preload" href="/pytorch-for-information-extraction/runtime~main.c5aed659.js" as="script">
<link rel="preload" href="/pytorch-for-information-extraction/main.fed981a0.js" as="script">
<link rel="preload" href="/pytorch-for-information-extraction/1.10db5ad8.js" as="script">
<link rel="preload" href="/pytorch-for-information-extraction/2.a4aec572.js" as="script">
<link rel="preload" href="/pytorch-for-information-extraction/17.cef2437d.js" as="script">
<link rel="preload" href="/pytorch-for-information-extraction/18.09f2fe56.js" as="script">
<link rel="preload" href="/pytorch-for-information-extraction/f976f453.5d9f15f5.js" as="script">
<link rel="preload" href="/pytorch-for-information-extraction/17896441.6e0c4643.js" as="script">
<link rel="preload" href="/pytorch-for-information-extraction/3.6b088b77.js" as="script">
<link rel="preload" href="/pytorch-for-information-extraction/09569cd7.9db6c608.js" as="script">
</head>
<body>
<script>!function(){function t(t){document.documentElement.setAttribute("data-theme",t)}var e=function(){var t=null;try{t=localStorage.getItem("theme")}catch(t){}return t}();t(null!==e?e:"light")}()</script><div id="__docusaurus">
<nav class="navbar navbar--fixed-top"><div class="navbar__inner"><div class="navbar__items"><div aria-label="Navigation bar toggle" class="navbar__toggle" role="button" tabindex="0"><svg xmlns="http://www.w3.org/2000/svg" width="30" height="30" viewBox="0 0 30 30" role="img" focusable="false"><title>Menu</title><path stroke="currentColor" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2" d="M4 7h22M4 15h22M4 23h22"></path></svg></div><a class="navbar__brand" href="/pytorch-for-information-extraction/"><img class="navbar__logo" src="/pytorch-for-information-extraction/img/logo.svg" alt="My Site Logo"><strong class="navbar__title">Pytorch For Information Extraction</strong></a></div><div class="navbar__items navbar__items--right"><a aria-current="page" class="navbar__item navbar__link demo-button navbar__link--active" href="/pytorch-for-information-extraction/">Demo</a><div class="react-toggle react-toggle--disabled displayOnlyInLargeViewport_2aTZ"><div class="react-toggle-track"><div class="react-toggle-track-check"><span class="toggle_BsTx">ðŸŒœ</span></div><div class="react-toggle-track-x"><span class="toggle_BsTx">ðŸŒž</span></div></div><div class="react-toggle-thumb"></div><input type="checkbox" disabled="" aria-label="Dark mode toggle" class="react-toggle-screenreader-only"></div></div></div><div role="presentation" class="navbar-sidebar__backdrop"></div><div class="navbar-sidebar"><div class="navbar-sidebar__brand"><a class="navbar__brand" href="/pytorch-for-information-extraction/"><img class="navbar__logo" src="/pytorch-for-information-extraction/img/logo.svg" alt="My Site Logo"><strong class="navbar__title">Pytorch For Information Extraction</strong></a></div><div class="navbar-sidebar__items"><div class="menu"><ul class="menu__list"><li class="menu__list-item"><a aria-current="page" class="menu__link demo-button navbar__link--active" href="/pytorch-for-information-extraction/">Demo</a></li></ul></div></div></div></nav><div class="main-wrapper"><div class="docPage_2gpo"><div class="docSidebarContainer_3_JD" role="complementary"><div class="sidebar_2urC"><div class="menu menu--responsive menu_5FrY"><button aria-label="Open Menu" aria-haspopup="true" class="button button--secondary button--sm menu__button" type="button"><svg aria-label="Menu" class="sidebarMenuIcon_Dm3K" xmlns="http://www.w3.org/2000/svg" height="24" width="24" viewBox="0 0 32 32" role="img" focusable="false"><title>Menu</title><path stroke="currentColor" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2" d="M4 7h22M4 15h22M4 23h22"></path></svg></button><ul class="menu__list"><li class="menu__list-item"><a class="menu__link" href="/pytorch-for-information-extraction/introduction">Getting Started</a></li><li class="menu__list-item"><a class="menu__link menu__link--sublist menu__link--active" href="#!">Modules</a><ul class="menu__list"><li class="menu__list-item"><a class="menu__link" tabindex="0" href="/pytorch-for-information-extraction/detection-module">1. Detection</a></li><li class="menu__list-item"><a aria-current="page" class="menu__link menu__link--active active" tabindex="0" href="/pytorch-for-information-extraction/orientation-module">2. Orientation</a></li><li class="menu__list-item"><a class="menu__link" tabindex="0" href="/pytorch-for-information-extraction/extraction-module">3. Extraction</a></li></ul></li><li class="menu__list-item"><a class="menu__link" href="/pytorch-for-information-extraction/conclusion/">Conclusion</a></li></ul></div></div></div><main class="docMainContainer_3EyW"><div class="container padding-vert--lg docItemWrapper_1EkI"><div class="row"><div class="col docItemCol_2ASc"><div class="docItemContainer_3QWW"><article><header><h1 class="docTitle_1Lrw">Orientation Module</h1></header><div class="markdown"><p>To predict the orientation of an aligned student-id image inputted from the detection module, we shall quickly develop an image classification model and train it on our <a href="https://github.com/MbassiJaphet/pytorch-for-information-extraction/tree/master/code/datasets/orientation" target="_blank" rel="noopener noreferrer">orientation dataset</a>. We expect the trained orientation model to predict the confidence scores for orientation angles (90, 180, 270, and 360) for an input image.</p><p>So, let&#x27;s resolve the imports of our orientation module.</p><h2><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2cZh" id="21-orientation-dataset"></a><strong>2.1. Orientation Dataset</strong><a aria-hidden="true" tabindex="-1" class="hash-link" href="#21-orientation-dataset" title="Direct link to heading">#</a></h2><p>The orientation datasets consist of folders containing four subfolders, whereby each subfolder is named according to one of the four orientation classes i.e. <strong>&#x27;090&#x27;</strong>, <strong>&#x27;180&#x27;</strong>, <strong>&#x27;270&#x27;</strong>, and <strong>&#x27;360&#x27;</strong>. Each subfolder contains images rotated according to their folder&#x27;s name.</p><p>Pytorch provides <a href="https://pytorch.org/docs/stable/torchvision/datasets.html#torchvision.datasets.ImageFolder" target="_blank" rel="noopener noreferrer">torchvision.datasets.ImageFolder</a> for loading datasets with such format without requiring us to hardcode a custom dataset class for the data like we did for the detection dataset.
<img alt="img" src="/pytorch-for-information-extraction/assets/images/orientation-datasets-15615fe8643c50cd5ad61e4b00fa8250.svg"></p><h3><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2cZh" id="211-define-transforms-for-orientation-datasets"></a>2.1.1. Define transforms for orientation datasets<a aria-hidden="true" tabindex="-1" class="hash-link" href="#211-define-transforms-for-orientation-datasets" title="Direct link to heading">#</a></h3><p>Before instantiating our various orientation datasets, we have to define the various transforms which shall be used to initialize them.</p><div class="mdxCodeBlock_1XEh"><code class="language-python"></code></div><h3><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2cZh" id="212--instantiate-orientation-datasets"></a>2.1.2.  Instantiate orientation datasets<a aria-hidden="true" tabindex="-1" class="hash-link" href="#212--instantiate-orientation-datasets" title="Direct link to heading">#</a></h3><p>We shall leverage Pytorch inbuilt torchvision.datasets.ImageFolder class to effortlessly instantiate our orientation training, validation, and testing datasets.</p><div class="mdxCodeBlock_1XEh"><code class="language-python"></code></div><p>Just checking the names and number of classes from our orientation dataset to make sure everything is <strong>OK</strong>!</p><pre class="output-block"><code></code></pre><h3><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2cZh" id="213-visualize-orientation-dataset"></a>2.1.3. Visualize orientation dataset<a aria-hidden="true" tabindex="-1" class="hash-link" href="#213-visualize-orientation-dataset" title="Direct link to heading">#</a></h3><div class="mdxCodeBlock_1XEh"><code class="language-python"></code></div><p><img alt="img" src="/pytorch-for-information-extraction/assets/images/orientation-sample-37cfa7cd053b4264cb2666797a86bd66.svg"></p><h2><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2cZh" id="22-orientation-model"></a><strong>2.2. Orientation Model</strong><a aria-hidden="true" tabindex="-1" class="hash-link" href="#22-orientation-model" title="Direct link to heading">#</a></h2><h3><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2cZh" id="221-define-orientation-model"></a>2.2.1. Define Orientation Model<a aria-hidden="true" tabindex="-1" class="hash-link" href="#221-define-orientation-model" title="Direct link to heading">#</a></h3><p><strong>Note</strong> that the model architecture defined below expects input image tensors of shape  <strong>(3 x 224 x 224)</strong> taking after transforms of the orientation datasets.</p><p>Let&#x27;s define an architecture for our orientation model from scratch.</p><pre style="display:block;overflow-x:auto;padding:0.5em;color:#abb2bf;background:#282c34;border-radius:8px;font-size:0.67em;overflow:auto;max-height:75vh"><code style="color:#e0e0e0;white-space:pre"></code></pre>Now that we have defined the architecture of our orientation model, let&#x27;s define the helper function to instantiate it ! ```python ```<h3><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2cZh" id="222-specify-checkpoint-and-instantiate-model"></a>2.2.2. Specify checkpoint and instantiate model<a aria-hidden="true" tabindex="-1" class="hash-link" href="#222-specify-checkpoint-and-instantiate-model" title="Direct link to heading">#</a></h3><p>Looking forward to <strong>resumable</strong> training and saving of our orientation model, we shall now specify the checkpoints for the <strong>state dictionaries</strong> of the model and its training optimizer.</p><div class="mdxCodeBlock_1XEh"><code class="language-python"></code></div><p>Let&#x27;s print our orientation to see if it has been initialized as we expect.</p><h2><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2cZh" id="23-training-and-evaluation"></a><strong>2.3. Training and Evaluation</strong><a aria-hidden="true" tabindex="-1" class="hash-link" href="#23-training-and-evaluation" title="Direct link to heading">#</a></h2><h3><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2cZh" id="231-specify-data-loaders"></a>2.3.1. Specify data loaders<a aria-hidden="true" tabindex="-1" class="hash-link" href="#231-specify-data-loaders" title="Direct link to heading">#</a></h3><p>After initializing the various orientation datasets, let us use them to specify data loaders which shall be used for training, validation, and testing.</p><div class="mdxCodeBlock_1XEh"><code class="language-python"></code></div><h3><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2cZh" id="232-define-loss-function-and-optimizer"></a>2.3.2. Define loss function and optimizer<a aria-hidden="true" tabindex="-1" class="hash-link" href="#232-define-loss-function-and-optimizer" title="Direct link to heading">#</a></h3><p>Let&#x27;s initialize the optimizer for training the orientation model, and get ready for training !</p><div class="mdxCodeBlock_1XEh"><code class="language-python"></code></div><h3><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2cZh" id="233-define-training-function"></a>2.3.3. Define training function<a aria-hidden="true" tabindex="-1" class="hash-link" href="#233-define-training-function" title="Direct link to heading">#</a></h3><div class="mdxCodeBlock_1XEh"><code class="language-python"></code></div><h3><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2cZh" id="234-train-orientation-model"></a>2.3.4. Train orientation model<a aria-hidden="true" tabindex="-1" class="hash-link" href="#234-train-orientation-model" title="Direct link to heading">#</a></h3><p>Now let&#x27;s train our orientation model for 30 epochs.</p><div class="mdxCodeBlock_1XEh"><code class="language-python"></code></div><h3><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2cZh" id="235-resume-training-orientation-model"></a>2.3.5. Resume training orientation model<a aria-hidden="true" tabindex="-1" class="hash-link" href="#235-resume-training-orientation-model" title="Direct link to heading">#</a></h3><p>At the end of every epoch, we had the checkpoints of the orientation module updated. Now let&#x27;s use these updated checkpoints to reload the orientation model with orientation optimizer and resume the training up to <strong>&#x27;30&#x27;</strong> epochs.</p><blockquote><p><strong>[IMPORTANT !!!]</strong> To reload the orientation model and the orientation optimizer from the checkpoint, simply re-run the code cells at Section 2.2.2. and Section 2.3.2 respectively. Just make sure <code>load_orientation_checkpoint</code> is set to <code>True</code>. The resulting output shall be identical to the one below.</p></blockquote><p>Reloading orientation model from the checkpoint. (Section 2.2.2)</p><div class="mdxCodeBlock_1XEh"><code class="language-python"></code></div><p>Reloading orientation optimizer from the checkpoint (Section 2.3.2)</p><div class="mdxCodeBlock_1XEh"><code class="language-python"></code></div><p>Now let&#x27;s resume training of our orientation model.</p><div class="mdxCodeBlock_1XEh"><code class="language-python"></code></div><p>You notice that the training start from epoch 21 since the orientation model has already been trained for 20 epochs.</p><h3><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2cZh" id="236-evaluate-orientation-model"></a>2.3.6. Evaluate orientation model<a aria-hidden="true" tabindex="-1" class="hash-link" href="#236-evaluate-orientation-model" title="Direct link to heading">#</a></h3><p>To conclude on the performance of your models, it is always of good practice to evaluate them on sample data.
We shall evaluate the performance of the orientation model on sample images from the testing dataset.</p><p>Firstly, let&#x27;s use our orientation model to predict the orientation of an input image from the test orientation dataset.</p><p>But, before that let&#x27;s define the test function.</p><div class="mdxCodeBlock_1XEh"><code class="language-python"></code></div><p>With our test function defined, we shall use it to evaluate the performance of the orientation model on the orientation test dataset.</p><div class="mdxCodeBlock_1XEh"><code class="language-python"></code></div><p>Secondly, let&#x27;s properly visualize the performance of our orientation model via inference on sample images from the test dataset one at a time.</p><p>Keep in mind that the objective behind an orientation module is to detect the orientation of an aligned document image, and to rectify it where necessary. Therefore, after inferencing every single image, we have shall apply the proper transformation to the image to rectify its orientation if necessary.</p><div class="mdxCodeBlock_1XEh"><code class="language-python"></code></div><p><img alt="img" src="/pytorch-for-information-extraction/assets/images/orientation-prediction-6d52d2a84333412809a9ae7e1cbb921b.svg"></p></div></article><div class="margin-vert--xl"><div class="row"><div class="col"><a href="https://github.com/MbassiJaphet/pytorch-for-information-extraction/edit/master/docs/orientation.md" target="_blank" rel="noreferrer noopener"><svg fill="currentColor" height="1.2em" width="1.2em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 40 40" style="margin-right:0.3em;vertical-align:sub"><g><path d="m34.5 11.7l-3 3.1-6.3-6.3 3.1-3q0.5-0.5 1.2-0.5t1.1 0.5l3.9 3.9q0.5 0.4 0.5 1.1t-0.5 1.2z m-29.5 17.1l18.4-18.5 6.3 6.3-18.4 18.4h-6.3v-6.2z"></path></g></svg>Edit this page</a></div></div></div><div class="margin-vert--lg"><nav class="pagination-nav" aria-label="Blog list page navigation"><div class="pagination-nav__item"><a class="pagination-nav__link" href="/pytorch-for-information-extraction/detection-module"><div class="pagination-nav__sublabel">Previous</div><div class="pagination-nav__label">Â« Detection Module</div></a></div><div class="pagination-nav__item pagination-nav__item--next"><a class="pagination-nav__link" href="/pytorch-for-information-extraction/extraction-module"><div class="pagination-nav__sublabel">Next</div><div class="pagination-nav__label">Extraction Module Â»</div></a></div></nav></div></div></div><div class="col col--3"><div class="tableOfContents_3SO_"><ul class="table-of-contents table-of-contents__left-border"><li><a href="#21-orientation-dataset" class="table-of-contents__link"><strong>2.1. Orientation Dataset</strong></a><ul><li><a href="#211-define-transforms-for-orientation-datasets" class="table-of-contents__link">2.1.1. Define transforms for orientation datasets</a></li><li><a href="#212--instantiate-orientation-datasets" class="table-of-contents__link">2.1.2.  Instantiate orientation datasets</a></li><li><a href="#213-visualize-orientation-dataset" class="table-of-contents__link">2.1.3. Visualize orientation dataset</a></li></ul></li><li><a href="#22-orientation-model" class="table-of-contents__link"><strong>2.2. Orientation Model</strong></a><ul><li><a href="#221-define-orientation-model" class="table-of-contents__link">2.2.1. Define Orientation Model</a></li><li><a href="#222-specify-checkpoint-and-instantiate-model" class="table-of-contents__link">2.2.2. Specify checkpoint and instantiate model</a></li></ul></li><li><a href="#23-training-and-evaluation" class="table-of-contents__link"><strong>2.3. Training and Evaluation</strong></a><ul><li><a href="#231-specify-data-loaders" class="table-of-contents__link">2.3.1. Specify data loaders</a></li><li><a href="#232-define-loss-function-and-optimizer" class="table-of-contents__link">2.3.2. Define loss function and optimizer</a></li><li><a href="#233-define-training-function" class="table-of-contents__link">2.3.3. Define training function</a></li><li><a href="#234-train-orientation-model" class="table-of-contents__link">2.3.4. Train orientation model</a></li><li><a href="#235-resume-training-orientation-model" class="table-of-contents__link">2.3.5. Resume training orientation model</a></li><li><a href="#236-evaluate-orientation-model" class="table-of-contents__link">2.3.6. Evaluate orientation model</a></li></ul></li></ul></div></div></div></div></main></div></div><footer class="footer footer--dark"><div class="container"><div class="row footer__links"><div class="col footer__col"><h4 class="footer__title">Get Started</h4><ul class="footer__items"><li class="footer__item"><a class="footer__link-item" href="/pytorch-for-information-extraction/introduction">Style Guide</a></li></ul></div><div class="col footer__col"><h4 class="footer__title">Community</h4><ul class="footer__items"><li class="footer__item"><a href="https://stackoverflow.com/questions/tagged/pytorch-for-information-extraction" target="_blank" rel="noopener noreferrer" class="footer__link-item">Stack Overflow</a></li></ul></div><div class="col footer__col"><h4 class="footer__title">More</h4><ul class="footer__items"><li class="footer__item"><a href="https://github.com/MbassiJaphet/pytorch-for-information-extraction" target="_blank" rel="noopener noreferrer" class="footer__link-item">GitHub</a></li></ul></div></div><div class="text--center"><div>Copyright Â© 2020 Pytorch For Information Extraction. Built with Docusaurus.</div></div></div></footer></div>
<script src="/pytorch-for-information-extraction/styles.bca2cbc6.js"></script>
<script src="/pytorch-for-information-extraction/runtime~main.c5aed659.js"></script>
<script src="/pytorch-for-information-extraction/main.fed981a0.js"></script>
<script src="/pytorch-for-information-extraction/1.10db5ad8.js"></script>
<script src="/pytorch-for-information-extraction/2.a4aec572.js"></script>
<script src="/pytorch-for-information-extraction/17.cef2437d.js"></script>
<script src="/pytorch-for-information-extraction/18.09f2fe56.js"></script>
<script src="/pytorch-for-information-extraction/f976f453.5d9f15f5.js"></script>
<script src="/pytorch-for-information-extraction/17896441.6e0c4643.js"></script>
<script src="/pytorch-for-information-extraction/3.6b088b77.js"></script>
<script src="/pytorch-for-information-extraction/09569cd7.9db6c608.js"></script>
</body>
</html>