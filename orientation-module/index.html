<!doctype html>
<html lang="en">
<head>
<meta charset="UTF-8">
<meta name="viewport" content="width=device-width,initial-scale=1">
<meta name="generator" content="Docusaurus v2.0.0-alpha.66">
<title data-react-helmet="true">Orientation Module | Pytorch for Information Extraction on Image Documents</title><meta data-react-helmet="true" name="twitter:card" content="summary_large_image"><meta data-react-helmet="true" name="docusaurus_language" content="en"><meta data-react-helmet="true" name="docusaurus_version" content="current"><meta data-react-helmet="true" name="docusaurus_tag" content="docs-default-current"><meta data-react-helmet="true" property="og:title" content="Orientation Module | Pytorch for Information Extraction on Image Documents"><meta data-react-helmet="true" name="description" content="To predict the orientation of an aligned student-id image inputted from the detection module, we shall quickly develop an image classification model and train it on our orientation dataset. We expect the trained orientation model to predict the confidence scores for orientation angles (90, 180, 270, and 360) for an input image."><meta data-react-helmet="true" property="og:description" content="To predict the orientation of an aligned student-id image inputted from the detection module, we shall quickly develop an image classification model and train it on our orientation dataset. We expect the trained orientation model to predict the confidence scores for orientation angles (90, 180, 270, and 360) for an input image."><meta data-react-helmet="true" property="og:url" content="https://mbassijaphet.github.io/pytorch-for-information-extraction//pytorch-for-information-extraction/orientation-module"><link data-react-helmet="true" rel="shortcut icon" href="/pytorch-for-information-extraction/img/favicon.ico"><link data-react-helmet="true" rel="canonical" href="https://mbassijaphet.github.io/pytorch-for-information-extraction//pytorch-for-information-extraction/orientation-module"><link rel="stylesheet" href="/pytorch-for-information-extraction/styles.103b7918.css">
<link rel="preload" href="/pytorch-for-information-extraction/styles.4239a677.js" as="script">
<link rel="preload" href="/pytorch-for-information-extraction/runtime~main.cff98e76.js" as="script">
<link rel="preload" href="/pytorch-for-information-extraction/main.a1916417.js" as="script">
<link rel="preload" href="/pytorch-for-information-extraction/1.e2d427d4.js" as="script">
<link rel="preload" href="/pytorch-for-information-extraction/3.878abcf8.js" as="script">
<link rel="preload" href="/pytorch-for-information-extraction/17.fe71ddf4.js" as="script">
<link rel="preload" href="/pytorch-for-information-extraction/18.39a0d9ac.js" as="script">
<link rel="preload" href="/pytorch-for-information-extraction/935f2afb.d50b60a2.js" as="script">
<link rel="preload" href="/pytorch-for-information-extraction/17896441.964af0ee.js" as="script">
<link rel="preload" href="/pytorch-for-information-extraction/2.cc85bb96.js" as="script">
<link rel="preload" href="/pytorch-for-information-extraction/e8b2bc97.f1aad1b9.js" as="script">
</head>
<body>
<script>!function(){function t(t){document.documentElement.setAttribute("data-theme",t)}var e=function(){var t=null;try{t=localStorage.getItem("theme")}catch(t){}return t}();t(null!==e?e:"light")}()</script><div id="__docusaurus">
<nav class="navbar navbar--fixed-top"><div class="navbar__inner"><div class="navbar__items"><div aria-label="Navigation bar toggle" class="navbar__toggle" role="button" tabindex="0"><svg xmlns="http://www.w3.org/2000/svg" width="30" height="30" viewBox="0 0 30 30" role="img" focusable="false"><title>Menu</title><path stroke="currentColor" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2" d="M4 7h22M4 15h22M4 23h22"></path></svg></div><a class="navbar__brand" href="/pytorch-for-information-extraction/"><img class="navbar__logo" src="/pytorch-for-information-extraction/img/logo.svg" alt="My Site Logo"><strong class="navbar__title">Pytorch for Information Extraction on Image Documents</strong></a></div><div class="navbar__items navbar__items--right"><a href="https://colab.research.google.com/github/MbassiJaphet/pytorch-for-information-extraction/blob/master/code/tutorial.ipynb" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link demo-button">Colab Version</a><div class="react-toggle react-toggle--disabled displayOnlyInLargeViewport_2aTZ"><div class="react-toggle-track"><div class="react-toggle-track-check"><span class="toggle_BsTx">ðŸŒœ</span></div><div class="react-toggle-track-x"><span class="toggle_BsTx">ðŸŒž</span></div></div><div class="react-toggle-thumb"></div><input type="checkbox" disabled="" aria-label="Dark mode toggle" class="react-toggle-screenreader-only"></div></div></div><div role="presentation" class="navbar-sidebar__backdrop"></div><div class="navbar-sidebar"><div class="navbar-sidebar__brand"><a class="navbar__brand" href="/pytorch-for-information-extraction/"><img class="navbar__logo" src="/pytorch-for-information-extraction/img/logo.svg" alt="My Site Logo"><strong class="navbar__title">Pytorch for Information Extraction on Image Documents</strong></a></div><div class="navbar-sidebar__items"><div class="menu"><ul class="menu__list"><li class="menu__list-item"><a href="https://colab.research.google.com/github/MbassiJaphet/pytorch-for-information-extraction/blob/master/code/tutorial.ipynb" target="_blank" rel="noopener noreferrer" class="menu__link demo-button">Colab Version</a></li></ul></div></div></div></nav><div class="main-wrapper"><div class="docPage_2gpo"><div class="docSidebarContainer_3_JD" role="complementary"><div class="sidebar_2urC"><div class="menu menu--responsive menu_5FrY"><button aria-label="Open Menu" aria-haspopup="true" class="button button--secondary button--sm menu__button" type="button"><svg aria-label="Menu" class="sidebarMenuIcon_Dm3K" xmlns="http://www.w3.org/2000/svg" height="24" width="24" viewBox="0 0 32 32" role="img" focusable="false"><title>Menu</title><path stroke="currentColor" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2" d="M4 7h22M4 15h22M4 23h22"></path></svg></button><ul class="menu__list"><li class="menu__list-item"><a class="menu__link" href="/pytorch-for-information-extraction/introduction">Getting Started</a></li><li class="menu__list-item"><a class="menu__link menu__link--sublist menu__link--active" href="#!">Modules</a><ul class="menu__list"><li class="menu__list-item"><a class="menu__link" tabindex="0" href="/pytorch-for-information-extraction/detection-module">1. Detection</a></li><li class="menu__list-item"><a aria-current="page" class="menu__link menu__link--active active" tabindex="0" href="/pytorch-for-information-extraction/orientation-module">2. Orientation</a></li><li class="menu__list-item"><a class="menu__link" tabindex="0" href="/pytorch-for-information-extraction/extraction-module">3. Extraction</a></li></ul></li><li class="menu__list-item"><a class="menu__link" href="/pytorch-for-information-extraction/conclusion/">Conclusion</a></li></ul></div></div></div><main class="docMainContainer_3EyW"><div class="container padding-vert--lg docItemWrapper_1EkI"><div class="row"><div class="col docItemCol_2ASc"><div class="docItemContainer_3QWW"><article><header><h1 class="docTitle_1Lrw">Orientation Module</h1></header><div class="markdown"><p>To predict the orientation of an aligned student-id image inputted from the detection module, we shall quickly develop an image classification model and train it on our <a href="https://github.com/MbassiJaphet/pytorch-for-information-extraction/tree/master/code/datasets/orientation" target="_blank" rel="noopener noreferrer">orientation dataset</a>. We expect the trained orientation model to predict the confidence scores for orientation angles (90, 180, 270, and 360) for an input image.</p><p>So, let&#x27;s resolve the imports of our orientation module.</p><div class="code-block"><pre style="display:block;overflow-x:auto;padding:0.5em;color:#abb2bf;background:#282c34;border-radius:8px;overflow:auto;max-height:75vh;font-size:0.67em"><code style="color:#e0e0e0;white-space:pre"></code></pre></div><h2><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2cZh" id="21-orientation-dataset"></a><strong>2.1. Orientation Dataset</strong><a aria-hidden="true" tabindex="-1" class="hash-link" href="#21-orientation-dataset" title="Direct link to heading">#</a></h2><p>The <a href="https://github.com/MbassiJaphet/pytorch-for-information-extraction/tree/master/code/datasets/orientation" target="_blank" rel="noopener noreferrer">orientation dataset</a> consist of folders containing four subfolders, whereby each subfolder is named according to one of the four orientation classes i.e. <strong>&#x27;090&#x27;</strong>, <strong>&#x27;180&#x27;</strong>, <strong>&#x27;270&#x27;</strong>, and <strong>&#x27;360&#x27;</strong>. Each subfolder contains images rotated according to their folder&#x27;s name.</p><p>Pytorch provides <a href="https://pytorch.org/docs/stable/torchvision/datasets.html#torchvision.datasets.ImageFolder" target="_blank" rel="noopener noreferrer">torchvision.datasets.ImageFolder</a> for loading datasets with such format without requiring us to hardcode a custom dataset class for the data like we did for the detection dataset.
<img alt="img" src="/pytorch-for-information-extraction/assets/images/orientation-datasets-15615fe8643c50cd5ad61e4b00fa8250.svg"></p><h3><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2cZh" id="211-define-transforms-for-orientation-datasets"></a>2.1.1. Define transforms for orientation datasets<a aria-hidden="true" tabindex="-1" class="hash-link" href="#211-define-transforms-for-orientation-datasets" title="Direct link to heading">#</a></h3><p>Before instantiating our various orientation datasets, we have to define the various transforms which shall be used to initialize them.</p><div class="code-block"><pre style="display:block;overflow-x:auto;padding:0.5em;color:#abb2bf;background:#282c34;border-radius:8px;overflow:auto;max-height:75vh;font-size:0.67em"><code style="color:#e0e0e0;white-space:pre"></code></pre></div><details open=""><summary>Code Brieffings</summary><ul><li>Defined and initialized various transforms specific to each of our orientation datasets(training, validation, and testing). We did that by <strong>importing</strong> and leveraging <a href="https://pytorch.org/docs/stable/torchvision/transforms.html?highlight=torchvision%20transforms" target="_blank" rel="noopener noreferrer">torchvision.transforms</a> which is a module containing common image transformations.</li><li><a href="https://pytorch.org/docs/stable/torchvision/transforms.html?highlight=transforms%20compose#torchvision.transforms.Compose" target="_blank" rel="noopener noreferrer">transforms.Compose</a> composes several transforms together.</li><li><a href="https://pytorch.org/docs/stable/torchvision/transforms.html?highlight=transforms%20resize#torchvision.transforms.Resize" target="_blank" rel="noopener noreferrer">transforms.Resize</a> resizes the input image to the given size.</li><li><a href="https://pytorch.org/docs/stable/torchvision/transforms.html?highlight=transforms%20randomaffine#torchvision.transforms.RandomAffine" target="_blank" rel="noopener noreferrer">transforms.RandomAffine</a> randomly affines transformation of the image keeping center invariant.</li><li><a href="https://pytorch.org/docs/stable/torchvision/transforms.html?highlight=transforms%20randomapply#torchvision.transforms.RandomApply" target="_blank" rel="noopener noreferrer">transforms.RandomApply</a> randomly a list of transformations with a given probability.</li><li><a href="https://pytorch.org/docs/stable/torchvision/transforms.html?highlight=transforms%20randomgrayscale#torchvision.transforms.RandomGrayscale" target="_blank" rel="noopener noreferrer">transforms.RandomGrayscale</a> randomly convert image to grayscale with a probability of <strong>&#x27;p&#x27;</strong>.</li><li><a href="https://pytorch.org/docs/stable/torchvision/transforms.html?highlight=transforms%20normalize#torchvision.transforms.Normalize" target="_blank" rel="noopener noreferrer">transforms.Normalize</a> normalize a tensor image with mean and standard deviation.</li><li><strong>Note</strong> that we only applied data augmentation on the training dataset. This is so that our model can easily generalize input data.</li></ul></details><h3><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2cZh" id="212--instantiate-orientation-datasets"></a>2.1.2.  Instantiate orientation datasets<a aria-hidden="true" tabindex="-1" class="hash-link" href="#212--instantiate-orientation-datasets" title="Direct link to heading">#</a></h3><p>We shall leverage Pytorch inbuilt torchvision.datasets.ImageFolder class to effortlessly instantiate our orientation training, validation, and testing datasets.</p><div class="code-block"><pre style="display:block;overflow-x:auto;padding:0.5em;color:#abb2bf;background:#282c34;border-radius:8px;overflow:auto;max-height:75vh;font-size:0.67em"><code style="color:#e0e0e0;white-space:pre"></code></pre></div><details open=""><summary>Code Brieffings</summary><ul><li>We initialized <strong>training</strong>, <strong>validation</strong>, and <strong>testing</strong> datasets using <a href="https://pytorch.org/docs/stable/torchvision/datasets.html#torchvision.datasets.ImageFolder" target="_blank" rel="noopener noreferrer">torchvision.datasets.ImageFolder</a> with their respective <strong>folders</strong>.</li><li>We initialized the variables<code>orientation_classes</code>, and <code>num_orientation_classes</code> to values of our <strong>orientation classes</strong> and their <strong>number</strong> respectively.</li></ul></details><p></p><p>Just checking the names and number of classes from our orientation dataset to make sure everything is <strong>OK</strong>!</p><div class="code-block"><pre style="display:block;overflow-x:auto;padding:0.5em;color:#abb2bf;background:#282c34;border-radius:8px;overflow:auto;max-height:75vh;font-size:0.67em"><code style="color:#e0e0e0;white-space:pre"></code></pre></div><pre class="output-block"><code></code></pre><h3><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2cZh" id="213-visualize-orientation-dataset"></a>2.1.3. Visualize orientation dataset<a aria-hidden="true" tabindex="-1" class="hash-link" href="#213-visualize-orientation-dataset" title="Direct link to heading">#</a></h3><div class="code-block"><pre style="display:block;overflow-x:auto;padding:0.5em;color:#abb2bf;background:#282c34;border-radius:8px;overflow:auto;max-height:75vh;font-size:0.67em"><code style="color:#e0e0e0;white-space:pre"></code></pre></div><details open=""><summary>Code Brieffings</summary><ul><li>Randomly selected a group of four elements from our orientation training dataset as <code>(image_tensor, label_tensor)</code> pairs.</li><li>Denormalized <code>image_tensor</code> for each pair and had each image plotted displaying their corresponding classes.</li></ul></details><p><img alt="img" src="/pytorch-for-information-extraction/assets/images/orientation-sample-75b7b005f45944ade1b10a6c2199641d.svg"></p><h2><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2cZh" id="22-orientation-model"></a><strong>2.2. Orientation Model</strong><a aria-hidden="true" tabindex="-1" class="hash-link" href="#22-orientation-model" title="Direct link to heading">#</a></h2><h3><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2cZh" id="221-define-orientation-model"></a>2.2.1. Define Orientation Model<a aria-hidden="true" tabindex="-1" class="hash-link" href="#221-define-orientation-model" title="Direct link to heading">#</a></h3><p><strong>Note</strong> that the model architecture defined below expects input image tensors of shape  <strong>(3 x 224 x 224)</strong> taking after transforms of the orientation datasets.
Let&#x27;s define an architecture for our orientation model from scratch.</p><div class="code-block"><pre style="display:block;overflow-x:auto;padding:0.5em;color:#abb2bf;background:#282c34;border-radius:8px;overflow:auto;max-height:75vh;font-size:0.67em"><code style="color:#e0e0e0;white-space:pre"></code></pre></div><details open=""><summary>Code Brieffings</summary><ul><li>Defined <code>OrientationModel</code> extending <a href="https://pytorch.org/docs/stable/generated/torch.nn.Module.html?highlight=torch%20nn%20module#torch.nn.Module" target="_blank" rel="noopener noreferrer">torch.nn.Module</a>. The constructor argument <code>num_classes</code> is equivalent to desired number of <strong>classes/labels</strong>.</li><li>Defined the feed-forward behavior of the neural network by overriding the <a href="https://pytorch.org/docs/stable/generated/torch.nn.Module.html?highlight=torch%20nn%20module%20forward#torch.nn.Module.forward" target="_blank" rel="noopener noreferrer"><code>forward</code></a> method.</li></ul></details><p>Now that we have defined the architecture of our orientation model, let&#x27;s define the helper function to instantiate it !</p><div class="code-block"><pre style="display:block;overflow-x:auto;padding:0.5em;color:#abb2bf;background:#282c34;border-radius:8px;overflow:auto;max-height:75vh;font-size:0.67em"><code style="color:#e0e0e0;white-space:pre"></code></pre></div><details open=""><summary>Code Brieffings</summary><ul><li>Used <a href="https://pytorch.org/docs/stable/generated/torch.nn.Module.html?highlight=load_state_dict#torch.nn.Module.load_state_dict" target="_blank" rel="noopener noreferrer">orientation_model.load_state_dict()</a> to set model weights from state dictionary if <code>state_dict</code> is given.</li></ul></details><h3><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2cZh" id="222-specify-checkpoint-and-instantiate-the-model"></a>2.2.2. Specify checkpoint and instantiate the model<a aria-hidden="true" tabindex="-1" class="hash-link" href="#222-specify-checkpoint-and-instantiate-the-model" title="Direct link to heading">#</a></h3><p>Looking forward to <strong>resumable</strong> training and saving of our orientation model, we shall now specify the checkpoints for the <strong>state dictionaries</strong> of the model and its training optimizer while initializing the model at once.</p><div class="code-block"><pre style="display:block;overflow-x:auto;padding:0.5em;color:#abb2bf;background:#282c34;border-radius:8px;overflow:auto;max-height:75vh;font-size:0.67em"><code style="color:#e0e0e0;white-space:pre"></code></pre></div><details open=""><summary>Code Brieffings</summary><ul><li>Selected available computational hardware using <a href="https://pytorch.org/docs/stable/tensor_attributes.html?highlight=torch%20device#torch.torch.device" target="_blank" rel="noopener noreferrer">torch.device()</a>.</li></ul></details><pre class="output-block"><code></code></pre><p>Now let&#x27;s print our orientation model to check if it has been initialized as we expect.</p><div class="code-block"><pre style="display:block;overflow-x:auto;padding:0.5em;color:#abb2bf;background:#282c34;border-radius:8px;overflow:auto;max-height:75vh;font-size:0.67em"><code style="color:#e0e0e0;white-space:pre"></code></pre></div><pre class="output-block"><code></code></pre><h2><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2cZh" id="23-training-and-validation"></a><strong>2.3. Training and Validation</strong><a aria-hidden="true" tabindex="-1" class="hash-link" href="#23-training-and-validation" title="Direct link to heading">#</a></h2><h3><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2cZh" id="231-specify-data-loaders"></a>2.3.1. Specify data loaders<a aria-hidden="true" tabindex="-1" class="hash-link" href="#231-specify-data-loaders" title="Direct link to heading">#</a></h3><p>After initializing the various orientation datasets, let us use them to specify data loaders which shall be used for training, validation, and testing.</p><div class="code-block"><pre style="display:block;overflow-x:auto;padding:0.5em;color:#abb2bf;background:#282c34;border-radius:8px;overflow:auto;max-height:75vh;font-size:0.67em"><code style="color:#e0e0e0;white-space:pre"></code></pre></div><details open=""><summary>Code Brieffings</summary><ul><li>Initialized data loaders for each of our orientation datasets (training, validation and testing) by using <a href="https://pytorch.org/docs/stable/data.html?highlight=torch%20utils%20data%20dataloader#torch.utils.data.DataLoader" target="_blank" rel="noopener noreferrer">torch.utils.data.DataLoader()</a>.</li><li>Initialized the dictionary variable &#x27;<code>orientation_loaders </code>&#x27;, which references all of the data loaders.</li></ul></details><h3><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2cZh" id="232-define-loss-function-and-optimizer"></a>2.3.2. Define loss function and optimizer<a aria-hidden="true" tabindex="-1" class="hash-link" href="#232-define-loss-function-and-optimizer" title="Direct link to heading">#</a></h3><p>Let&#x27;s initialize the optimizer for training the orientation model, and get ready for training !</p><div class="code-block"><pre style="display:block;overflow-x:auto;padding:0.5em;color:#abb2bf;background:#282c34;border-radius:8px;overflow:auto;max-height:75vh;font-size:0.67em"><code style="color:#e0e0e0;white-space:pre"></code></pre></div><pre class="output-block"><code></code></pre><h3><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2cZh" id="233-define-training-function"></a>2.3.3. Define training function<a aria-hidden="true" tabindex="-1" class="hash-link" href="#233-define-training-function" title="Direct link to heading">#</a></h3><div class="code-block"><pre style="display:block;overflow-x:auto;padding:0.5em;color:#abb2bf;background:#282c34;border-radius:8px;overflow:auto;max-height:75vh;font-size:0.67em"><code style="color:#e0e0e0;white-space:pre"></code></pre></div><details open=""><summary>Code Brieffings</summary><ul><li>Moved the <strong>model</strong> to the computation device as the <code>(data, target)</code> pairs from <code>loaders[&#x27;train&#x27;]</code>.</li><li>Within the training loop, we reset the gradients before predicting <code>output</code> for each <code>data</code>, and its compute <code>loss</code> to its <code>target</code>.</li><li>Find best <code>loss</code> as <code>valid_loss</code> and update checkpoint accordingly.</li></ul></details><h3><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2cZh" id="234-train-orientation-model"></a>2.3.4. Train orientation model<a aria-hidden="true" tabindex="-1" class="hash-link" href="#234-train-orientation-model" title="Direct link to heading">#</a></h3><p>Now let&#x27;s train our orientation model for 20 epochs.</p><div class="code-block"><pre style="display:block;overflow-x:auto;padding:0.5em;color:#abb2bf;background:#282c34;border-radius:8px;overflow:auto;max-height:75vh;font-size:0.67em"><code style="color:#e0e0e0;white-space:pre"></code></pre></div><pre class="output-block"><code></code></pre><h3><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2cZh" id="235-resume-training-orientation-model"></a>2.3.5. Resume training orientation model<a aria-hidden="true" tabindex="-1" class="hash-link" href="#235-resume-training-orientation-model" title="Direct link to heading">#</a></h3><p>At the end of every epoch, we had the checkpoints of the orientation module updated. Now let&#x27;s use these updated checkpoints to reload the orientation model with orientation optimizer and resume the training up to <strong>&#x27;30&#x27;</strong> epochs.</p><div class="admonition admonition-important alert alert--info"><div class="admonition-heading"><h5><span class="admonition-icon"><svg xmlns="http://www.w3.org/2000/svg" width="14" height="16" viewBox="0 0 14 16"><path fill-rule="evenodd" d="M7 2.3c3.14 0 5.7 2.56 5.7 5.7s-2.56 5.7-5.7 5.7A5.71 5.71 0 0 1 1.3 8c0-3.14 2.56-5.7 5.7-5.7zM7 1C3.14 1 0 4.14 0 8s3.14 7 7 7 7-3.14 7-7-3.14-7-7-7zm1 3H6v5h2V4zm0 6H6v2h2v-2z"></path></svg></span>important</h5></div><div class="admonition-content"><p>To reload the orientation model and the orientation optimizer from the checkpoint, simply re-run the code cells in Section 2.2.2. and Section 2.3.2 respectively. Just make sure <code>load_orientation_checkpoint</code> is set to <code>True</code>. The resulting outputs shall be identical to the ones below.</p></div></div><p>Reloading orientation model from the checkpoint. (Section 2.2.2)</p><div class="code-block"><pre style="display:block;overflow-x:auto;padding:0.5em;color:#abb2bf;background:#282c34;border-radius:8px;overflow:auto;max-height:75vh;font-size:0.67em"><code style="color:#e0e0e0;white-space:pre"></code></pre></div><details open=""><summary>Code Brieffings</summary><ul><li>Loaded checkpoint using <a href="https://pytorch.org/docs/stable/generated/torch.load.html?highlight=torch%20load#torch.load" target="_blank" rel="noopener noreferrer">torch.load()</a>. The argument <code>map_location</code> is used to specify the computing device into which the checkpoint is loaded.</li></ul></details><pre class="output-block"><code></code></pre><p>Reloading orientation optimizer from the checkpoint (Section 2.3.2)</p><div class="code-block"><pre style="display:block;overflow-x:auto;padding:0.5em;color:#abb2bf;background:#282c34;border-radius:8px;overflow:auto;max-height:75vh;font-size:0.67em"><code style="color:#e0e0e0;white-space:pre"></code></pre></div><details open=""><summary>Code Brieffings</summary><ul><li>Used <a href="https://pytorch.org/docs/stable/generated/torch.nn.Module.html?highlight=load_state_dict#torch.nn.Module.load_state_dict" target="_blank" rel="noopener noreferrer">orientation_optimizer.load_state_dict()</a> to initialize optimizer weights if <code>orientation_optimizer_state_dict</code> is available. This sets the optimizer to  the state after that of the previous training.</li></ul></details><pre class="output-block"><code></code></pre><p>Now let&#x27;s resume the training of our orientation model.</p><div class="code-block"><pre style="display:block;overflow-x:auto;padding:0.5em;color:#abb2bf;background:#282c34;border-radius:8px;overflow:auto;max-height:75vh;font-size:0.67em"><code style="color:#e0e0e0;white-space:pre"></code></pre></div><pre class="output-block"><code></code></pre><p>You notice that the training starts from <strong>epoch 21</strong> since the orientation model has already been trained for <strong>20 epochs</strong>.</p><h3><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2cZh" id="236-evaluate-orientation-model"></a>2.3.6. Evaluate orientation model<a aria-hidden="true" tabindex="-1" class="hash-link" href="#236-evaluate-orientation-model" title="Direct link to heading">#</a></h3><p>To conclude on the performance of your models, it is always of good practice to evaluate them on sample data. We shall evaluate the performance of the orientation model on sample images from the testing dataset.</p><p>But, before that let&#x27;s define the test function.</p><div class="code-block"><pre style="display:block;overflow-x:auto;padding:0.5em;color:#abb2bf;background:#282c34;border-radius:8px;overflow:auto;max-height:75vh;font-size:0.67em"><code style="color:#e0e0e0;white-space:pre"></code></pre></div><details open=""><summary>Code Brieffings</summary><ul><li>Put the model to evaluation mode using <a href="https://pytorch.org/docs/stable/generated/torch.nn.Module.html?highlight=torch%20nn%20module%20eval#torch.nn.Module.eval" target="_blank" rel="noopener noreferrer">model.eval()</a>. This disables some training behaviors of our model such as <strong>batch normalization</strong> and dropout layers.</li><li>Iterate <strong>batches</strong> of the <strong>orientation test loader</strong> for <code>(data, target)</code> pairs.</li><li>Move <code>(data, target)</code> pairs to computation <strong>device/hardware</strong> using <a href="https://pytorch.org/docs/stable/tensors.html?highlight=torch%20tensor#torch.Tensor.to" target="_blank" rel="noopener noreferrer">to()</a> method.</li><li>Predict <code>output</code> for <code>data</code> and compute <code>loss</code> to targets. Then the average total loss is computed as<code>test_loss</code>.</li></ul></details><p>With our test function defined, we shall now use it to evaluate the performance of the orientation model on the orientation test dataset.</p><div class="code-block"><pre style="display:block;overflow-x:auto;padding:0.5em;color:#abb2bf;background:#282c34;border-radius:8px;overflow:auto;max-height:75vh;font-size:0.67em"><code style="color:#e0e0e0;white-space:pre"></code></pre></div><pre class="output-block"><code></code></pre><h2><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2cZh" id="24-orientation-correction"></a>2.4 Orientation Correction<a aria-hidden="true" tabindex="-1" class="hash-link" href="#24-orientation-correction" title="Direct link to heading">#</a></h2><p>Let&#x27;s properly visualize the performance of our orientation model via inference on sample images from the test dataset one at a time.</p><p>Keep in mind that the objective behind an orientation module is to detect the orientation of an aligned document image, and to rectify it where necessary. Therefore, after inferencing every single image, we have shall apply the proper transformation to the image to rectify its orientation if necessary.</p><div class="code-block"><pre style="display:block;overflow-x:auto;padding:0.5em;color:#abb2bf;background:#282c34;border-radius:8px;overflow:auto;max-height:75vh;font-size:0.67em"><code style="color:#e0e0e0;white-space:pre"></code></pre></div><p><img alt="img" src="/pytorch-for-information-extraction/assets/images/orientation-prediction-e0d5d34f70eec00b2d293c49a7adf0de.svg"></p><details><summary>More Outputs</summary><p><img alt="img" src="/pytorch-for-information-extraction/assets/images/orientation-prediction-1-4a5b71d4f5648358e8d004e5cf4cccd4.svg"></p><p></p><p><img alt="img" src="/pytorch-for-information-extraction/assets/images/orientation-prediction-2-db6b70e4daeb1ca70ec55509ea75504c.svg"></p></details></div></article><div class="margin-vert--xl"><div class="row"><div class="col"><a href="https://github.com/MbassiJaphet/pytorch-for-information-extraction/edit/master/docs/orientation.md" target="_blank" rel="noreferrer noopener"><svg fill="currentColor" height="1.2em" width="1.2em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 40 40" style="margin-right:0.3em;vertical-align:sub"><g><path d="m34.5 11.7l-3 3.1-6.3-6.3 3.1-3q0.5-0.5 1.2-0.5t1.1 0.5l3.9 3.9q0.5 0.4 0.5 1.1t-0.5 1.2z m-29.5 17.1l18.4-18.5 6.3 6.3-18.4 18.4h-6.3v-6.2z"></path></g></svg>Edit this page</a></div></div></div><div class="margin-vert--lg"><nav class="pagination-nav" aria-label="Blog list page navigation"><div class="pagination-nav__item"><a class="pagination-nav__link" href="/pytorch-for-information-extraction/detection-module"><div class="pagination-nav__sublabel">Previous</div><div class="pagination-nav__label">Â« Detection Module</div></a></div><div class="pagination-nav__item pagination-nav__item--next"><a class="pagination-nav__link" href="/pytorch-for-information-extraction/extraction-module"><div class="pagination-nav__sublabel">Next</div><div class="pagination-nav__label">Extraction Module Â»</div></a></div></nav></div></div></div><div class="col col--3"><div class="tableOfContents_3SO_"><ul class="table-of-contents table-of-contents__left-border"><li><a href="#21-orientation-dataset" class="table-of-contents__link"><strong>2.1. Orientation Dataset</strong></a><ul><li><a href="#211-define-transforms-for-orientation-datasets" class="table-of-contents__link">2.1.1. Define transforms for orientation datasets</a></li><li><a href="#212--instantiate-orientation-datasets" class="table-of-contents__link">2.1.2.  Instantiate orientation datasets</a></li><li><a href="#213-visualize-orientation-dataset" class="table-of-contents__link">2.1.3. Visualize orientation dataset</a></li></ul></li><li><a href="#22-orientation-model" class="table-of-contents__link"><strong>2.2. Orientation Model</strong></a><ul><li><a href="#221-define-orientation-model" class="table-of-contents__link">2.2.1. Define Orientation Model</a></li><li><a href="#222-specify-checkpoint-and-instantiate-the-model" class="table-of-contents__link">2.2.2. Specify checkpoint and instantiate the model</a></li></ul></li><li><a href="#23-training-and-validation" class="table-of-contents__link"><strong>2.3. Training and Validation</strong></a><ul><li><a href="#231-specify-data-loaders" class="table-of-contents__link">2.3.1. Specify data loaders</a></li><li><a href="#232-define-loss-function-and-optimizer" class="table-of-contents__link">2.3.2. Define loss function and optimizer</a></li><li><a href="#233-define-training-function" class="table-of-contents__link">2.3.3. Define training function</a></li><li><a href="#234-train-orientation-model" class="table-of-contents__link">2.3.4. Train orientation model</a></li><li><a href="#235-resume-training-orientation-model" class="table-of-contents__link">2.3.5. Resume training orientation model</a></li><li><a href="#236-evaluate-orientation-model" class="table-of-contents__link">2.3.6. Evaluate orientation model</a></li></ul></li><li><a href="#24-orientation-correction" class="table-of-contents__link">2.4 Orientation Correction</a></li></ul></div></div></div></div></main></div></div><footer class="footer footer--dark"><div class="container"><div class="row footer__links"><div class="col footer__col"><h4 class="footer__title">Get Started</h4><ul class="footer__items"><li class="footer__item"><a class="footer__link-item" href="/pytorch-for-information-extraction/introduction">Style Guide</a></li></ul></div><div class="col footer__col"><h4 class="footer__title">Community</h4><ul class="footer__items"><li class="footer__item"><a href="https://stackoverflow.com/questions/tagged/pytorch-for-information-extraction" target="_blank" rel="noopener noreferrer" class="footer__link-item">Stack Overflow</a></li></ul></div><div class="col footer__col"><h4 class="footer__title">More</h4><ul class="footer__items"><li class="footer__item"><a href="https://github.com/MbassiJaphet/pytorch-for-information-extraction" target="_blank" rel="noopener noreferrer" class="footer__link-item">GitHub</a></li></ul></div></div><div class="text--center"><div>Copyright Â© 2020 Pytorch For Information Extraction. Built with Docusaurus.</div></div></div></footer></div>
<script src="/pytorch-for-information-extraction/styles.4239a677.js"></script>
<script src="/pytorch-for-information-extraction/runtime~main.cff98e76.js"></script>
<script src="/pytorch-for-information-extraction/main.a1916417.js"></script>
<script src="/pytorch-for-information-extraction/1.e2d427d4.js"></script>
<script src="/pytorch-for-information-extraction/3.878abcf8.js"></script>
<script src="/pytorch-for-information-extraction/17.fe71ddf4.js"></script>
<script src="/pytorch-for-information-extraction/18.39a0d9ac.js"></script>
<script src="/pytorch-for-information-extraction/935f2afb.d50b60a2.js"></script>
<script src="/pytorch-for-information-extraction/17896441.964af0ee.js"></script>
<script src="/pytorch-for-information-extraction/2.cc85bb96.js"></script>
<script src="/pytorch-for-information-extraction/e8b2bc97.f1aad1b9.js"></script>
</body>
</html>