<!doctype html>
<html lang="en">
<head>
<meta charset="UTF-8">
<meta name="viewport" content="width=device-width,initial-scale=1">
<meta name="generator" content="Docusaurus v2.0.0-alpha.66">
<title data-react-helmet="true">Detection Module | Pytorch for Information Extraction on Image Documents</title><meta data-react-helmet="true" name="twitter:card" content="summary_large_image"><meta data-react-helmet="true" name="docusaurus_language" content="en"><meta data-react-helmet="true" name="docusaurus_version" content="current"><meta data-react-helmet="true" name="docusaurus_tag" content="docs-default-current"><meta data-react-helmet="true" property="og:title" content="Detection Module | Pytorch for Information Extraction on Image Documents"><meta data-react-helmet="true" name="description" content="To locate student-id(s) within images, we gonna leverage transfer learning via fine-tuning the state of art object segmentation algorithm Mask R-CNN  backboned by pre-trained ResNet-50 available in torchvision models gallery."><meta data-react-helmet="true" property="og:description" content="To locate student-id(s) within images, we gonna leverage transfer learning via fine-tuning the state of art object segmentation algorithm Mask R-CNN  backboned by pre-trained ResNet-50 available in torchvision models gallery."><meta data-react-helmet="true" property="og:url" content="https://mbassijaphet.github.io/pytorch-for-information-extraction//pytorch-for-information-extraction/detection-module"><link data-react-helmet="true" rel="shortcut icon" href="/pytorch-for-information-extraction/img/favicon.ico"><link data-react-helmet="true" rel="canonical" href="https://mbassijaphet.github.io/pytorch-for-information-extraction//pytorch-for-information-extraction/detection-module"><link rel="stylesheet" href="/pytorch-for-information-extraction/styles.103b7918.css">
<link rel="preload" href="/pytorch-for-information-extraction/styles.4239a677.js" as="script">
<link rel="preload" href="/pytorch-for-information-extraction/runtime~main.6f681a46.js" as="script">
<link rel="preload" href="/pytorch-for-information-extraction/main.a1916417.js" as="script">
<link rel="preload" href="/pytorch-for-information-extraction/1.e2d427d4.js" as="script">
<link rel="preload" href="/pytorch-for-information-extraction/3.878abcf8.js" as="script">
<link rel="preload" href="/pytorch-for-information-extraction/17.fe71ddf4.js" as="script">
<link rel="preload" href="/pytorch-for-information-extraction/18.39a0d9ac.js" as="script">
<link rel="preload" href="/pytorch-for-information-extraction/935f2afb.d50b60a2.js" as="script">
<link rel="preload" href="/pytorch-for-information-extraction/17896441.964af0ee.js" as="script">
<link rel="preload" href="/pytorch-for-information-extraction/2.9d5854b4.js" as="script">
<link rel="preload" href="/pytorch-for-information-extraction/22a58971.0ba1782e.js" as="script">
</head>
<body>
<script>!function(){function t(t){document.documentElement.setAttribute("data-theme",t)}var e=function(){var t=null;try{t=localStorage.getItem("theme")}catch(t){}return t}();t(null!==e?e:"light")}()</script><div id="__docusaurus">
<nav class="navbar navbar--fixed-top"><div class="navbar__inner"><div class="navbar__items"><div aria-label="Navigation bar toggle" class="navbar__toggle" role="button" tabindex="0"><svg xmlns="http://www.w3.org/2000/svg" width="30" height="30" viewBox="0 0 30 30" role="img" focusable="false"><title>Menu</title><path stroke="currentColor" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2" d="M4 7h22M4 15h22M4 23h22"></path></svg></div><a class="navbar__brand" href="/pytorch-for-information-extraction/"><img class="navbar__logo" src="/pytorch-for-information-extraction/img/logo.svg" alt="My Site Logo"><strong class="navbar__title">Pytorch for Information Extraction on Image Documents</strong></a></div><div class="navbar__items navbar__items--right"><a href="https://colab.research.google.com/github/MbassiJaphet/pytorch-for-information-extraction/blob/master/code/tutorial.ipynb" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link demo-button">Colab Version</a><div class="react-toggle react-toggle--disabled displayOnlyInLargeViewport_2aTZ"><div class="react-toggle-track"><div class="react-toggle-track-check"><span class="toggle_BsTx">ðŸŒœ</span></div><div class="react-toggle-track-x"><span class="toggle_BsTx">ðŸŒž</span></div></div><div class="react-toggle-thumb"></div><input type="checkbox" disabled="" aria-label="Dark mode toggle" class="react-toggle-screenreader-only"></div></div></div><div role="presentation" class="navbar-sidebar__backdrop"></div><div class="navbar-sidebar"><div class="navbar-sidebar__brand"><a class="navbar__brand" href="/pytorch-for-information-extraction/"><img class="navbar__logo" src="/pytorch-for-information-extraction/img/logo.svg" alt="My Site Logo"><strong class="navbar__title">Pytorch for Information Extraction on Image Documents</strong></a></div><div class="navbar-sidebar__items"><div class="menu"><ul class="menu__list"><li class="menu__list-item"><a href="https://colab.research.google.com/github/MbassiJaphet/pytorch-for-information-extraction/blob/master/code/tutorial.ipynb" target="_blank" rel="noopener noreferrer" class="menu__link demo-button">Colab Version</a></li></ul></div></div></div></nav><div class="main-wrapper"><div class="docPage_2gpo"><div class="docSidebarContainer_3_JD" role="complementary"><div class="sidebar_2urC"><div class="menu menu--responsive menu_5FrY"><button aria-label="Open Menu" aria-haspopup="true" class="button button--secondary button--sm menu__button" type="button"><svg aria-label="Menu" class="sidebarMenuIcon_Dm3K" xmlns="http://www.w3.org/2000/svg" height="24" width="24" viewBox="0 0 32 32" role="img" focusable="false"><title>Menu</title><path stroke="currentColor" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2" d="M4 7h22M4 15h22M4 23h22"></path></svg></button><ul class="menu__list"><li class="menu__list-item"><a class="menu__link" href="/pytorch-for-information-extraction/introduction">Getting Started</a></li><li class="menu__list-item"><a class="menu__link menu__link--sublist menu__link--active" href="#!">Modules</a><ul class="menu__list"><li class="menu__list-item"><a aria-current="page" class="menu__link menu__link--active active" tabindex="0" href="/pytorch-for-information-extraction/detection-module">1. Detection</a></li><li class="menu__list-item"><a class="menu__link" tabindex="0" href="/pytorch-for-information-extraction/orientation-module">2. Orientation</a></li><li class="menu__list-item"><a class="menu__link" tabindex="0" href="/pytorch-for-information-extraction/extraction-module">3. Extraction</a></li></ul></li><li class="menu__list-item"><a class="menu__link" href="/pytorch-for-information-extraction/conclusion/">Conclusion</a></li></ul></div></div></div><main class="docMainContainer_3EyW"><div class="container padding-vert--lg docItemWrapper_1EkI"><div class="row"><div class="col docItemCol_2ASc"><div class="docItemContainer_3QWW"><article><header><h1 class="docTitle_1Lrw">Detection Module</h1></header><div class="markdown"><p>To locate student-id(s) within images, we gonna leverage transfer learning via fine-tuning the state of art object segmentation algorithm <a href="https://arxiv.org/abs/1703.06870" target="_blank" rel="noopener noreferrer"><strong>Mask R-CNN</strong></a>  backboned by pre-trained <a href="https://pytorch.org/docs/stable/torchvision/models.html#mask-r-cnn" target="_blank" rel="noopener noreferrer"><strong>ResNet-50</strong></a> available in torchvision models gallery.</p><p>So, let&#x27;s resolve the imports of our detection module.</p><div class="code-block"><pre style="display:block;overflow-x:auto;padding:0.5em;color:#abb2bf;background:#282c34;border-radius:8px;overflow:auto;max-height:75vh;font-size:0.67em"><code style="color:#e0e0e0;white-space:pre"></code></pre></div><details open=""><summary>Code Brieffing</summary><ul><li>Import <a href="https://pytorch.org/docs/stable/torch.html" target="_blank" rel="noopener noreferrer">torch</a> and <a href="https://pytorch.org/docs/stable/torchvision/" target="_blank" rel="noopener noreferrer">torchvision</a> which are libraries of the Pytorch project.</li><li><a href="https://pytorch.org/docs/stable/torchvision/transforms.html?highlight=torchvision%20transforms%20totensor#torchvision.transforms.ToTensor" target="_blank" rel="noopener noreferrer">torchvision.transforms.ToTensor()</a> reurns a function which takes in a PIL image and converts it to a <a href="https://pytorch.org/docs/stable/generated/torch.tensor.html?highlight=tensor#torch.tensor" target="_blank" rel="noopener noreferrer">tensor</a>.</li><li><a href="https://pytorch.org/docs/stable/torchvision/transforms.html?highlight=torchvision%20transforms%20topilimage#torchvision.transforms.ToPILImage" target="_blank" rel="noopener noreferrer">torchvision.transforms.ToPILImage()</a> reurns a function which that does the opposite.</li></ul></details><h2><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2cZh" id="11-detection-dataset"></a><strong>1.1. Detection Dataset</strong><a aria-hidden="true" tabindex="-1" class="hash-link" href="#11-detection-dataset" title="Direct link to heading">#</a></h2><h3><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2cZh" id="111-define-dataset-class"></a>1.1.1. Define dataset class<a aria-hidden="true" tabindex="-1" class="hash-link" href="#111-define-dataset-class" title="Direct link to heading">#</a></h3><p>A crucial requirement when fine-tuning, training, or inferencing models in Pytorch is to know the exact formats of data that specific models expect as inputs and compute as outputs.</p><p>The input to the model is expected to be a list of tensors, each of shape <code>[C, H, W]</code>, one for each image, and should be in the range<code>0-1</code> . Different images can have different sizes.</p><p><strong>Let&#x27;s take a look at the format of targets expected by the model</strong>.</p><ul><li>boxes (<code>FloatTensor[N, 4]</code>): the ground-truth boxes in <code>[x1, y1, x2, y2]</code> format, with values of <code>x</code> between <code>0</code> and <code>W</code> and values of y between <code>0</code> and <code>H</code>.</li><li>labels (<code>Int64Tensor[N]</code>): the class label for each ground-truth box.</li><li>masks (<code>UInt8Tensor[N, H, W]</code>): the segmentation binary masks for each instance.</li></ul><p><strong>Then, we shall also take a look at the format of outputs predicted by the model</strong>.</p><ul><li>boxes (<code>FloatTensor[N, 4]</code>): the predicted boxes in <code>[x1, y1, x2, y2]</code> format, with values of <code>x</code> between <code>0</code> and <code>W</code> and values of <code>y</code> between <code>0</code> and <code>H</code>.</li><li>labels (<code>Int64Tensor[N]</code>): the predicted labels for each image.</li><li>scores (<code>Tensor[N]</code>): the scores or each prediction.</li><li>masks (<code>UInt8Tensor[N, 1, H, W]</code>): the predicted masks for each instance, in the range <code>0-1</code>. To obtain the final segmentation masks, the soft masks can be thresholded, generally with a value of <code>0.5</code> (<code>mask &gt;= 0.5</code>).</li></ul><p>Recall from the <a href="/pytorch-for-information-extraction/introduction/#project-description/">project description</a> that we shall train our detection model on the <a href="https://github.com/MbassiJaphet/pytorch-for-information-extraction/tree/master/code/datasets/detection" target="_blank" rel="noopener noreferrer"><strong>Student-ID</strong></a> dataset. So letâ€™s examine its format !
<img alt="img" src="/pytorch-for-information-extraction/assets/images/detection-datasets-ac8430fb8f4be3539a0ad271ce519111.svg"></p><p>Now, knowing the formats of the Student-ID dataset as well as the formats of inputs/targets/outputs of the pre-trained model, we can confidently code a custom dataset class inheriting from <a href="https://pytorch.org/docs/stable/data.html#torch.utils.data.Dataset" target="_blank" rel="noopener noreferrer">torch.utils.data.Dataset</a>.</p><div class="code-block"><pre style="display:block;overflow-x:auto;padding:0.5em;color:#abb2bf;background:#282c34;border-radius:8px;overflow:auto;max-height:75vh;font-size:0.67em"><code style="color:#e0e0e0;white-space:pre"></code></pre></div><details open=""><summary>Code Brieffing</summary><ul><li>We defined the <code>DetectionDataset</code> class initialized with <code>data_path</code>(folder containing detection dataset), a <code>mode</code>(<strong>&#x27;TRAIN&#x27;</strong>, <strong>&#x27;VALID&#x27;</strong>, <strong>&#x27;TEST&#x27;</strong>), and <code>transform</code> (data augmentation function).</li><li>We implicitly assigned anything but our <code>classes</code> to the <strong>&#x27;BACKGROUND&#x27;</strong> class.</li><li>We implemented <a href="https://pytorch.org/docs/stable/data.html?highlight=torch%20utils%20data%20dataset#torch.utils.data.Dataset" target="_blank" rel="noopener noreferrer"><code>__getitem__</code></a> to return individual elements of our dataset as (<code>image_tensor</code>, <code>targets</code>) pairs.</li><li><a href="https://pytorch.org/docs/stable/generated/torch.from_numpy.html?highlight=torch%20from_numpy#torch.from_numpy" target="_blank" rel="noopener noreferrer">torch.from_numpy()</a> Creates a Tensor from a numpy.ndarray</li><li><a href="https://pytorch.org/docs/stable/generated/torch.as_tensor.html?highlight=torch%20as_tensor#torch.as_tensor" target="_blank" rel="noopener noreferrer">torch.as_tensor()</a> Convert the data into a torch.Tensor</li></ul></details><h3><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2cZh" id="112-define-transforms-for-detection-dataset"></a>1.1.2. Define transforms for detection dataset<a aria-hidden="true" tabindex="-1" class="hash-link" href="#112-define-transforms-for-detection-dataset" title="Direct link to heading">#</a></h3><p>Let&#x27;s write some helper functions for data augmentation.</p><div class="code-block"><pre style="display:block;overflow-x:auto;padding:0.5em;color:#abb2bf;background:#282c34;border-radius:8px;overflow:auto;max-height:75vh;font-size:0.67em"><code style="color:#e0e0e0;white-space:pre"></code></pre></div><h3><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2cZh" id="113-instantiate-detection-datasets"></a>1.1.3. Instantiate detection datasets<a aria-hidden="true" tabindex="-1" class="hash-link" href="#113-instantiate-detection-datasets" title="Direct link to heading">#</a></h3><div class="code-block"><pre style="display:block;overflow-x:auto;padding:0.5em;color:#abb2bf;background:#282c34;border-radius:8px;overflow:auto;max-height:75vh;font-size:0.67em"><code style="color:#e0e0e0;white-space:pre"></code></pre></div><details open=""><summary>Code Brieffings</summary><ul><li>We initialized <strong>training</strong>, <strong>validation</strong>, and <strong>testing</strong> datasets using the modes &#x27;TRAIN&#x27;, &#x27;VALID&#x27; and &#x27;TEST&#x27; respectively.</li><li>We <strong>disabled</strong> data augmentation for testing dataset.</li><li>We initialized the variables<code>detection_classes</code>, and <code>num_detection_classes</code> to values of our <strong>detection classes</strong> and their <strong>number</strong> respectively.</li></ul></details><p></p><p>Just checking the names and number of classes from our detection dataset to make sure everything is <strong>OK</strong>!</p><div class="code-block"><pre style="display:block;overflow-x:auto;padding:0.5em;color:#abb2bf;background:#282c34;border-radius:8px;overflow:auto;max-height:75vh;font-size:0.67em"><code style="color:#e0e0e0;white-space:pre"></code></pre></div><pre class="output-block"><code></code></pre><h3><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2cZh" id="114-visualize-detection-dataset"></a>1.1.4. Visualize detection dataset<a aria-hidden="true" tabindex="-1" class="hash-link" href="#114-visualize-detection-dataset" title="Direct link to heading">#</a></h3><div class="code-block"><pre style="display:block;overflow-x:auto;padding:0.5em;color:#abb2bf;background:#282c34;border-radius:8px;overflow:auto;max-height:75vh;font-size:0.67em"><code style="color:#e0e0e0;white-space:pre"></code></pre></div><details open=""><summary>Code Brieffings</summary><ul><li>We selected an inidividual element from <code>detection_train_set</code> using <code>id</code> as (<code>image_tensor</code>, <code>targets</code>) pairs.</li><li>We retrieved bounding boxes, segmentation masks, and labels from the <code>targets</code> dictionary.</li><li><a href="https://pytorch.org/docs/stable/generated/torch.zeros_like.html?highlight=torch%20zeros_like#torch.zeros_like" target="_blank" rel="noopener noreferrer">torch.zeros_like()</a> returns a tensor filled <code>0s</code>, with the same size as input.</li><li><a href="https://pytorch.org/docs/stable/tensors.html?highlight=tensor%20item#torch.Tensor.item" target="_blank" rel="noopener noreferrer">torch.Tensor.item()</a> returns the value of this tensor as a standard Python number.</li></ul></details><p><img alt="img" src="/pytorch-for-information-extraction/assets/images/detection-sample-2297f873c30fffa91d665d132959c6b2.svg"></p><h2><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2cZh" id="12-detection-model"></a><strong>1.2. Detection Model</strong><a aria-hidden="true" tabindex="-1" class="hash-link" href="#12-detection-model" title="Direct link to heading">#</a></h2><h3><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2cZh" id="121-define-detection-model"></a>1.2.1. Define detection model<a aria-hidden="true" tabindex="-1" class="hash-link" href="#121-define-detection-model" title="Direct link to heading">#</a></h3><p>Let&#x27;s define a helper function to instantiate the detection model !</p><div class="code-block"><pre style="display:block;overflow-x:auto;padding:0.5em;color:#abb2bf;background:#282c34;border-radius:8px;overflow:auto;max-height:75vh;font-size:0.67em"><code style="color:#e0e0e0;white-space:pre"></code></pre></div><details open=""><summary>Code Brieffings</summary><ul><li>We imported <a href="https://pytorch.org/docs/stable/torchvision/models.html?highlight=torchvision%20models%20detection%20faster_rcnn#mask-r-cnn" target="_blank" rel="noopener noreferrer">Mask-RCNN predictor</a> and <a href="https://pytorch.org/docs/stable/torchvision/models.html?highlight=torchvision%20models%20detection%20faster_rcnn#faster-r-cnn" target="_blank" rel="noopener noreferrer">Fast-RCNN predictor</a> heads.</li><li>Loaded <strong>Mask R-CNN model</strong> with pre-trained <strong>ResNet-50-FPN</strong> backbone and <strong>finetuned</strong> it using <code>num_classes</code>. Using the pre-trained model implicitly makes us use <a href="https://en.wikipedia.org/wiki/Transfer_learning" target="_blank" rel="noopener noreferrer"><strong>transfer learning</strong></a> which in turn makes our model converge faster.</li><li>Used <a href="https://pytorch.org/docs/stable/generated/torch.nn.Module.html?highlight=load_state_dict#torch.nn.Module.load_state_dict" target="_blank" rel="noopener noreferrer">detection_model.load_state_dict()</a> to set model weights from state dictionary if <code>state_dict</code> is given.</li></ul></details><blockquote><p><strong>Remark:</strong> The helper function above allows us to fine-tune the pre-trained <strong>FastRCNNPredictor</strong> and <strong>MaskRCNNPredictor</strong> with the desired number of classes, which are <strong>&#x27;2&#x27;</strong> in our case i.e. for the &#x27;BACKGROUND&#x27; and &#x27;Student_ID&#x27; classes. The function also sets the number of hidden layers of <strong>MaskRCNNPredictor</strong> to <strong>&#x27;256&#x27;</strong> but we can decide to tweak that for the best of our model performance.</p></blockquote><h3><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2cZh" id="122-specify-checkpoints-and-instantiate-the-model"></a>1.2.2. Specify checkpoints and instantiate the model<a aria-hidden="true" tabindex="-1" class="hash-link" href="#122-specify-checkpoints-and-instantiate-the-model" title="Direct link to heading">#</a></h3><p>Looking forward to <strong>resumable</strong> training and saving of our detection model, we shall now specify the checkpoints for the <strong>state dictionaries</strong> for both the model and its training optimizer.</p><div class="code-block"><pre style="display:block;overflow-x:auto;padding:0.5em;color:#abb2bf;background:#282c34;border-radius:8px;overflow:auto;max-height:75vh;font-size:0.67em"><code style="color:#e0e0e0;white-space:pre"></code></pre></div><details open=""><summary>Code Brieffings</summary><ul><li>Selected available computational hardware using <a href="https://pytorch.org/docs/stable/tensor_attributes.html?highlight=torch%20device#torch.torch.device" target="_blank" rel="noopener noreferrer">torch.device()</a>.</li><li><a href="https://pytorch.org/docs/stable/cuda.html?highlight=torch%20cuda%20is_available#torch.cuda.is_available" target="_blank" rel="noopener noreferrer">torch.cuda.is_available()</a> returns <code>True</code> if cuda capable hardware(s) is/are found.</li><li>Loaded checkpoints using <a href="https://pytorch.org/docs/stable/generated/torch.load.html?highlight=torch%20load#torch.load" target="_blank" rel="noopener noreferrer">torch.load()</a>. The argument <code>map_location</code> is used to specify the computing device into which the checkpoint is loaded. This very useful if you have no idea of the device type for which a tensor has been saved.</li></ul></details><pre class="output-block"><code></code></pre><h2><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2cZh" id="13-training-and-evaluation"></a><strong>1.3. Training and Evaluation</strong><a aria-hidden="true" tabindex="-1" class="hash-link" href="#13-training-and-evaluation" title="Direct link to heading">#</a></h2><p><strong>Note</strong> that the files used for training and validation of detection module found <code>./modules/detection/scripts</code> folder were directly copied along with their dependencies from torchvision reference detection training scripts repository.</p><h3><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2cZh" id="131-specify-data-loaders"></a>1.3.1. Specify data loaders<a aria-hidden="true" tabindex="-1" class="hash-link" href="#131-specify-data-loaders" title="Direct link to heading">#</a></h3><p>After initializing the various detection datasets, let us use them to specify data loaders which shall be used for training, validation, and testing.</p><div class="code-block"><pre style="display:block;overflow-x:auto;padding:0.5em;color:#abb2bf;background:#282c34;border-radius:8px;overflow:auto;max-height:75vh;font-size:0.67em"><code style="color:#e0e0e0;white-space:pre"></code></pre></div><details open=""><summary>Code Brieffings</summary><ul><li>Initialized data loaders for each of our detection datasets (training, validation and testing) by using <a href="https://pytorch.org/docs/stable/data.html?highlight=torch%20utils%20data%20dataloader#torch.utils.data.DataLoader" target="_blank" rel="noopener noreferrer">torch.utils.data.DataLoader()</a>.</li><li>Initialized the dictionary variable &#x27;<code>detection_loaders </code>&#x27;, which references all of the data loaders.</li></ul></details><h3><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2cZh" id="132-initialize-optimizer"></a>1.3.2. Initialize optimizer<a aria-hidden="true" tabindex="-1" class="hash-link" href="#132-initialize-optimizer" title="Direct link to heading">#</a></h3><p>Let&#x27;s initialize the optimizer for training the detection model, and get ready for training !</p><div class="code-block"><pre style="display:block;overflow-x:auto;padding:0.5em;color:#abb2bf;background:#282c34;border-radius:8px;overflow:auto;max-height:75vh;font-size:0.67em"><code style="color:#e0e0e0;white-space:pre"></code></pre></div><pre class="output-block"><code></code></pre><h3><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2cZh" id="133-define-training-function"></a>1.3.3. Define training function<a aria-hidden="true" tabindex="-1" class="hash-link" href="#133-define-training-function" title="Direct link to heading">#</a></h3><p>Now, let&#x27;s write the function that will train and validate our model for us. Inside the training function, we shall add a few lines of code that will save our model checkpoints.</p><div class="code-block"><pre style="display:block;overflow-x:auto;padding:0.5em;color:#abb2bf;background:#282c34;border-radius:8px;overflow:auto;max-height:75vh;font-size:0.67em"><code style="color:#e0e0e0;white-space:pre"></code></pre></div><h3><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2cZh" id="134-train-detection-model"></a>1.3.4 Train detection model<a aria-hidden="true" tabindex="-1" class="hash-link" href="#134-train-detection-model" title="Direct link to heading">#</a></h3><p>So letâ€™s train our detection model for 20 epochs saving it at the end of each epoch.</p><div class="code-block"><pre style="display:block;overflow-x:auto;padding:0.5em;color:#abb2bf;background:#282c34;border-radius:8px;overflow:auto;max-height:75vh;font-size:0.67em"><code style="color:#e0e0e0;white-space:pre"></code></pre></div><pre class="output-block"><code></code></pre><h3><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2cZh" id="135-resume-training-detection-model"></a>1.3.5. Resume training detection model<a aria-hidden="true" tabindex="-1" class="hash-link" href="#135-resume-training-detection-model" title="Direct link to heading">#</a></h3><p>At the end of every epoch, we had the checkpoints of the detection module updated. Now let&#x27;s use these updated checkpoints to reload the detection model and resume its training up to <strong>&#x27;30&#x27;</strong> epochs.</p><div class="admonition admonition-important alert alert--info"><div class="admonition-heading"><h5><span class="admonition-icon"><svg xmlns="http://www.w3.org/2000/svg" width="14" height="16" viewBox="0 0 14 16"><path fill-rule="evenodd" d="M7 2.3c3.14 0 5.7 2.56 5.7 5.7s-2.56 5.7-5.7 5.7A5.71 5.71 0 0 1 1.3 8c0-3.14 2.56-5.7 5.7-5.7zM7 1C3.14 1 0 4.14 0 8s3.14 7 7 7 7-3.14 7-7-3.14-7-7-7zm1 3H6v5h2V4zm0 6H6v2h2v-2z"></path></svg></span>important</h5></div><div class="admonition-content"><p>To reload the detection model and the detection optimizer from the checkpoint, simply re-run the code cells in Section 1.2.2. and Section 1.3.2 respectively. Just make sure <code>load_detection_checkpoint</code> is set to <code>True</code>. The resulting outputs shall be identical to the ones below.</p></div></div><p>Reloading detection model from the checkpoint. (Section 1.2.2)</p><div class="code-block"><pre style="display:block;overflow-x:auto;padding:0.5em;color:#abb2bf;background:#282c34;border-radius:8px;overflow:auto;max-height:75vh;font-size:0.67em"><code style="color:#e0e0e0;white-space:pre"></code></pre></div><details open=""><summary>Code Brieffings</summary><ul><li>Loaded checkpoint using <a href="https://pytorch.org/docs/stable/generated/torch.load.html?highlight=torch%20load#torch.load" target="_blank" rel="noopener noreferrer">torch.load()</a>. The argument <code>map_location</code> is used to specify the computing device into which the checkpoint is loaded.</li></ul></details><pre class="output-block"><code></code></pre><p>Reloading detection optimizer from the checkpoint (Section 1.3.2)</p><div class="code-block"><pre style="display:block;overflow-x:auto;padding:0.5em;color:#abb2bf;background:#282c34;border-radius:8px;overflow:auto;max-height:75vh;font-size:0.67em"><code style="color:#e0e0e0;white-space:pre"></code></pre></div><details open=""><summary>Code Brieffings</summary><ul><li>Used <a href="https://pytorch.org/docs/stable/generated/torch.nn.Module.html?highlight=load_state_dict#torch.nn.Module.load_state_dict" target="_blank" rel="noopener noreferrer">detection_optimizer.load_state_dict()</a> to initialize optimizer weights if <code>detection_optimizer_state_dict</code> is available. This sets the optimizer to  the state after that of the previous training.</li></ul></details><pre class="output-block"><code></code></pre><p>Now let&#x27;s resume training of our detection model.</p><div class="code-block"><pre style="display:block;overflow-x:auto;padding:0.5em;color:#abb2bf;background:#282c34;border-radius:8px;overflow:auto;max-height:75vh;font-size:0.67em"><code style="color:#e0e0e0;white-space:pre"></code></pre></div><pre class="output-block"><code></code></pre><p>You notice that the training start from epoch <strong>21</strong> since the detection model has already been trained for 20 epochs.</p><h3><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2cZh" id="136-evaluate-the-detection-model"></a>1.3.6. Evaluate the detection model<a aria-hidden="true" tabindex="-1" class="hash-link" href="#136-evaluate-the-detection-model" title="Direct link to heading">#</a></h3><p>To conclude on the performance of your models, it is always of good practice to evaluate them on sample data. We shall evaluate the performance of the detection model on sample images from the testing dataset.</p><p>Firstly, let&#x27;s use our detection model to compute predictions for an input image from the test detection dataset.</p><div class="code-block"><pre style="display:block;overflow-x:auto;padding:0.5em;color:#abb2bf;background:#282c34;border-radius:8px;overflow:auto;max-height:75vh;font-size:0.67em"><code style="color:#e0e0e0;white-space:pre"></code></pre></div><details open=""><summary>Code Brieffings</summary><ul><li>Selected an image URL from the testing dataset for inference.</li><li>Then put the model to evaluation mode using <a href="https://pytorch.org/docs/stable/generated/torch.nn.Module.html?highlight=torch%20nn%20module%20eval#torch.nn.Module.eval" target="_blank" rel="noopener noreferrer">detection_model.eval()</a>. That disables training features like batch normalization and dropout making inference faster.</li><li>Disabled gradient calculations for operations on tensors <strong>within a block</strong> using <a href="https://pytorch.org/docs/stable/generated/torch.no_grad.html?highlight=torch%20no_grad#torch.no_grad" target="_blank" rel="noopener noreferrer"><code>with torch.no_grad():</code></a>.</li></ul></details><p><img alt="img" src="/pytorch-for-information-extraction/assets/images/student-id-01-5375f7492350b2bca5e95746be434e85.svg"></p><p>Secondly, let&#x27;s take a look at the raw outputs predicted by our detection model for the image above.</p><div class="code-block"><pre style="display:block;overflow-x:auto;padding:0.5em;color:#abb2bf;background:#282c34;border-radius:8px;overflow:auto;max-height:75vh;font-size:0.67em"><code style="color:#e0e0e0;white-space:pre"></code></pre></div><pre class="output-block"><code></code></pre><p>As we can see the predictions are simply a dictionary containing <strong>labels</strong>, <strong>scores</strong>, <strong>boxes</strong>, and <strong>masks</strong> of detected objects in tensor format.</p><p></p><p>Lastly, let&#x27;s convert the raw predicted outputs into a human-understandable format for proper visualization.</p><div class="code-block"><pre style="display:block;overflow-x:auto;padding:0.5em;color:#abb2bf;background:#282c34;border-radius:8px;overflow:auto;max-height:75vh;font-size:0.67em"><code style="color:#e0e0e0;white-space:pre"></code></pre></div><p><img alt="img" src="/pytorch-for-information-extraction/assets/images/detection-prediction-3858b74e87735a5587fe77b5c146f5db.svg"></p><details><summary>More Outputs</summary><p><img alt="img" src="/pytorch-for-information-extraction/assets/images/detection-prediction-1-9c3a1f985f13d2b285adc339b53b6512.svg">
<img alt="img" src="/pytorch-for-information-extraction/assets/images/detection-prediction-2-312c4b46db7819d52b6526af1c77c6cc.svg"></p></details><h2><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2cZh" id="14-student-id-alignment"></a><strong>1.4. Student ID Alignment</strong><a aria-hidden="true" tabindex="-1" class="hash-link" href="#14-student-id-alignment" title="Direct link to heading">#</a></h2><p>At this point, what is left to be done in this module is to align student-id(s) detected by out detection model. The aligned student-id(s) shall then be fed as input to the orientation module.</p><div class="code-block"><pre style="display:block;overflow-x:auto;padding:0.5em;color:#abb2bf;background:#282c34;border-radius:8px;overflow:auto;max-height:75vh;font-size:0.67em"><code style="color:#e0e0e0;white-space:pre"></code></pre></div><p><img alt="img" src="/pytorch-for-information-extraction/assets/images/image-alignment-fa0084d9b2299b6f93223064468e8933.svg">
Now, let&#x27;s save our aligned student-id.</p><div class="code-block"><pre style="display:block;overflow-x:auto;padding:0.5em;color:#abb2bf;background:#282c34;border-radius:8px;overflow:auto;max-height:75vh;font-size:0.67em"><code style="color:#e0e0e0;white-space:pre"></code></pre></div></div></article><div class="margin-vert--xl"><div class="row"><div class="col"><a href="https://github.com/MbassiJaphet/pytorch-for-information-extraction/edit/master/docs/detection.md" target="_blank" rel="noreferrer noopener"><svg fill="currentColor" height="1.2em" width="1.2em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 40 40" style="margin-right:0.3em;vertical-align:sub"><g><path d="m34.5 11.7l-3 3.1-6.3-6.3 3.1-3q0.5-0.5 1.2-0.5t1.1 0.5l3.9 3.9q0.5 0.4 0.5 1.1t-0.5 1.2z m-29.5 17.1l18.4-18.5 6.3 6.3-18.4 18.4h-6.3v-6.2z"></path></g></svg>Edit this page</a></div></div></div><div class="margin-vert--lg"><nav class="pagination-nav" aria-label="Blog list page navigation"><div class="pagination-nav__item"><a class="pagination-nav__link" href="/pytorch-for-information-extraction/introduction"><div class="pagination-nav__sublabel">Previous</div><div class="pagination-nav__label">Â« Getting Started</div></a></div><div class="pagination-nav__item pagination-nav__item--next"><a class="pagination-nav__link" href="/pytorch-for-information-extraction/orientation-module"><div class="pagination-nav__sublabel">Next</div><div class="pagination-nav__label">Orientation Module Â»</div></a></div></nav></div></div></div><div class="col col--3"><div class="tableOfContents_3SO_"><ul class="table-of-contents table-of-contents__left-border"><li><a href="#11-detection-dataset" class="table-of-contents__link"><strong>1.1. Detection Dataset</strong></a><ul><li><a href="#111-define-dataset-class" class="table-of-contents__link">1.1.1. Define dataset class</a></li><li><a href="#112-define-transforms-for-detection-dataset" class="table-of-contents__link">1.1.2. Define transforms for detection dataset</a></li><li><a href="#113-instantiate-detection-datasets" class="table-of-contents__link">1.1.3. Instantiate detection datasets</a></li><li><a href="#114-visualize-detection-dataset" class="table-of-contents__link">1.1.4. Visualize detection dataset</a></li></ul></li><li><a href="#12-detection-model" class="table-of-contents__link"><strong>1.2. Detection Model</strong></a><ul><li><a href="#121-define-detection-model" class="table-of-contents__link">1.2.1. Define detection model</a></li><li><a href="#122-specify-checkpoints-and-instantiate-the-model" class="table-of-contents__link">1.2.2. Specify checkpoints and instantiate the model</a></li></ul></li><li><a href="#13-training-and-evaluation" class="table-of-contents__link"><strong>1.3. Training and Evaluation</strong></a><ul><li><a href="#131-specify-data-loaders" class="table-of-contents__link">1.3.1. Specify data loaders</a></li><li><a href="#132-initialize-optimizer" class="table-of-contents__link">1.3.2. Initialize optimizer</a></li><li><a href="#133-define-training-function" class="table-of-contents__link">1.3.3. Define training function</a></li><li><a href="#134-train-detection-model" class="table-of-contents__link">1.3.4 Train detection model</a></li><li><a href="#135-resume-training-detection-model" class="table-of-contents__link">1.3.5. Resume training detection model</a></li><li><a href="#136-evaluate-the-detection-model" class="table-of-contents__link">1.3.6. Evaluate the detection model</a></li></ul></li><li><a href="#14-student-id-alignment" class="table-of-contents__link"><strong>1.4. Student ID Alignment</strong></a></li></ul></div></div></div></div></main></div></div><footer class="footer footer--dark"><div class="container"><div class="row footer__links"><div class="col footer__col"><h4 class="footer__title">Get Started</h4><ul class="footer__items"><li class="footer__item"><a class="footer__link-item" href="/pytorch-for-information-extraction/introduction">Style Guide</a></li></ul></div><div class="col footer__col"><h4 class="footer__title">Community</h4><ul class="footer__items"><li class="footer__item"><a href="https://stackoverflow.com/questions/tagged/pytorch-for-information-extraction" target="_blank" rel="noopener noreferrer" class="footer__link-item">Stack Overflow</a></li></ul></div><div class="col footer__col"><h4 class="footer__title">More</h4><ul class="footer__items"><li class="footer__item"><a href="https://github.com/MbassiJaphet/pytorch-for-information-extraction" target="_blank" rel="noopener noreferrer" class="footer__link-item">GitHub</a></li></ul></div></div><div class="text--center"><div>Copyright Â© 2020 Pytorch For Information Extraction. Built with Docusaurus.</div></div></div></footer></div>
<script src="/pytorch-for-information-extraction/styles.4239a677.js"></script>
<script src="/pytorch-for-information-extraction/runtime~main.6f681a46.js"></script>
<script src="/pytorch-for-information-extraction/main.a1916417.js"></script>
<script src="/pytorch-for-information-extraction/1.e2d427d4.js"></script>
<script src="/pytorch-for-information-extraction/3.878abcf8.js"></script>
<script src="/pytorch-for-information-extraction/17.fe71ddf4.js"></script>
<script src="/pytorch-for-information-extraction/18.39a0d9ac.js"></script>
<script src="/pytorch-for-information-extraction/935f2afb.d50b60a2.js"></script>
<script src="/pytorch-for-information-extraction/17896441.964af0ee.js"></script>
<script src="/pytorch-for-information-extraction/2.9d5854b4.js"></script>
<script src="/pytorch-for-information-extraction/22a58971.0ba1782e.js"></script>
</body>
</html>