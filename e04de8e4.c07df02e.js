(window.webpackJsonp=window.webpackJsonp||[]).push([[12],{331:function(e,t,o){"use strict";o.r(t),t.default=o.p+"assets/images/detection-datasets-66bcf358dca91491b78492a44e010a0f.svg"},332:function(e,t,o){"use strict";o.r(t),t.default=o.p+"assets/images/detection-sample-2297f873c30fffa91d665d132959c6b2.svg"},333:function(e,t,o){"use strict";o.r(t),t.default=o.p+"assets/images/student-id-01-4636266652e4b32f00b75bf894324013.svg"},334:function(e,t,o){"use strict";o.r(t),t.default=o.p+"assets/images/detection-prediction-140d791c826f5968ee109c7cd3661f33.svg"},335:function(e,t,o){"use strict";o.r(t),t.default=o.p+"assets/images/image-alignment-0e5cbddee6adae8aff723dd8f4867986.svg"},67:function(e,t,o){"use strict";o.r(t),o.d(t,"frontMatter",(function(){return r})),o.d(t,"metadata",(function(){return s})),o.d(t,"rightToc",(function(){return l})),o.d(t,"default",(function(){return u}));var i=o(2),n=o(6),a=(o(0),o(73)),c=o(87),d=o(88),r={id:"detection",sidebar_label:"1. Detection",title:"Detection Module",slug:"/detection-module"},s={unversionedId:"detection",id:"detection",isDocsHomePage:!1,title:"Detection Module",description:"To localize student-id(s) in images, we gonna fine-tune the state of art object segmentation algorithm Mask R-CNN  on top of pre-trained ResNet-50 available in torchvision models repository.",source:"@site/docs\\detection.md",slug:"/detection-module",permalink:"/pytorch-for-information-extraction/detection-module",editUrl:"https://github.com/MbassiJaphet/pytorch-for-information-extraction/edit/master/docs/detection.md",version:"current",sidebar_label:"1. Detection",sidebar:"tutorial",previous:{title:"Getting Started",permalink:"/pytorch-for-information-extraction/introduction"},next:{title:"Orientation Module",permalink:"/pytorch-for-information-extraction/orientation-module"}},l=[{value:"<strong>1.1. Detection Dataset</strong>",id:"11-detection-dataset",children:[{value:"1.1.1. Define dataset class",id:"111-define-dataset-class",children:[]},{value:"1.1.2. Define transforms for detection dataset",id:"112-define-transforms-for-detection-dataset",children:[]},{value:"1.1.3. Instantiate detection datasets",id:"113-instantiate-detection-datasets",children:[]},{value:"1.1.4. Visualize detection dataset",id:"114-visualize-detection-dataset",children:[]}]},{value:"<strong>1.2. Detection Model</strong>",id:"12-detection-model",children:[{value:"1.2.1. Define detection model",id:"121-define-detection-model",children:[]},{value:"1.2.2. Specify checkpoints and instantiate model",id:"122-specify-checkpoints-and-instantiate-model",children:[]}]},{value:"<strong>1.3. Training and Evaluation</strong>",id:"13-training-and-evaluation",children:[{value:"1.3.1. Specify data loaders",id:"131-specify-data-loaders",children:[]},{value:"1.3.2. Initialize optimizer",id:"132-initialize-optimizer",children:[]},{value:"1.3.3. Define training function",id:"133-define-training-function",children:[]},{value:"1.3.4 Train detection model",id:"134-train-detection-model",children:[]},{value:"1.3.5. Resume training detection model",id:"135-resume-training-detection-model",children:[]},{value:"1.3.6. Evaluate the detection model",id:"136-evaluate-the-detection-model",children:[]}]},{value:"<strong>1.4. Preparation for Orientation Module</strong>",id:"14-preparation-for-orientation-module",children:[]}],p={rightToc:l};function u(e){var t=e.components,r=Object(n.a)(e,["components"]);return Object(a.b)("wrapper",Object(i.a)({},p,r,{components:t,mdxType:"MDXLayout"}),Object(a.b)("p",null,"To localize student-id(s) in images, we gonna fine-tune the state of art object segmentation algorithm ",Object(a.b)("strong",{parentName:"p"},"Mask R-CNN"),"  on top of pre-trained ",Object(a.b)("a",Object(i.a)({parentName:"p"},{href:"https://arxiv.org/abs/1703.06870"}),Object(a.b)("strong",{parentName:"a"},"ResNet-50"))," available in ",Object(a.b)("a",Object(i.a)({parentName:"p"},{href:"https://pytorch.org/docs/stable/torchvision/models.html#mask-r-cnn"}),"torchvision")," models repository."),Object(a.b)("p",null,"So, let's resolve the imports of our detection module."),Object(a.b)(c.a,{file:"detection_module_imports",mdxType:"CodeBlock"}),Object(a.b)("h2",{id:"11-detection-dataset"},Object(a.b)("strong",{parentName:"h2"},"1.1. Detection Dataset")),Object(a.b)("h3",{id:"111-define-dataset-class"},"1.1.1. Define dataset class"),Object(a.b)("p",null,"Recall from the ",Object(a.b)("a",Object(i.a)({parentName:"p"},{href:"/introduction/#project-description/"}),"project description")," that we shall train our detection model on the ",Object(a.b)("a",Object(i.a)({parentName:"p"},{href:"https://github.com/MbassiJaphet/pytorch-for-information-extraction/tree/master/code/datasets/detection"}),Object(a.b)("strong",{parentName:"a"},"Student-ID"))," dataset. So let\u2019s examine its format !"),Object(a.b)("p",null,"A crucial requirement when fine-tuning, training, or inferencing models in Pytorch is to know the exact formats of data that specific models expect as inputs and compute as outputs.\n",Object(a.b)("img",{alt:"img",src:o(331).default}),"\nNow, knowing the formats of the Student-ID dataset as well as formats of inputs and outputs of Mask R-CNN, we can confidently code a custom dataset class inheriting from ",Object(a.b)("a",Object(i.a)({parentName:"p"},{href:"https://pytorch.org/docs/stable/data.html#torch.utils.data.Dataset"}),"torch.utils.data.Dataset"),"."),Object(a.b)(c.a,{file:"detection_dataset",mdxType:"CodeBlock"}),Object(a.b)("h3",{id:"112-define-transforms-for-detection-dataset"},"1.1.2. Define transforms for detection dataset"),Object(a.b)("p",null,"Let's write some helper functions for data augmentation."),Object(a.b)(c.a,{file:"detection_dataset_transforms",mdxType:"CodeBlock"}),Object(a.b)("h3",{id:"113-instantiate-detection-datasets"},"1.1.3. Instantiate detection datasets"),Object(a.b)(c.a,{file:"detection_dataset_init",mdxType:"CodeBlock"}),Object(a.b)("p",null,"Just checking the names and number of classes from our detection dataset to make sure everything is ",Object(a.b)("strong",{parentName:"p"},"OK"),"!"),Object(a.b)(c.a,{file:"detection_dataset_classes",mdxType:"CodeBlock"}),Object(a.b)(d.a,{file:"detection_dataset_classes_output",mdxType:"OutputBlock"}),Object(a.b)("h3",{id:"114-visualize-detection-dataset"},"1.1.4. Visualize detection dataset"),Object(a.b)(c.a,{file:"detection_dataset_visualize",mdxType:"CodeBlock"}),Object(a.b)("p",null,Object(a.b)("img",{alt:"img",src:o(332).default})),Object(a.b)("h2",{id:"12-detection-model"},Object(a.b)("strong",{parentName:"h2"},"1.2. Detection Model")),Object(a.b)("h3",{id:"121-define-detection-model"},"1.2.1. Define detection model"),Object(a.b)("p",null,"Let's define a helper function to instantiate the detection model !"),Object(a.b)(c.a,{file:"detection_model_init_function",mdxType:"CodeBlock"}),Object(a.b)("blockquote",null,Object(a.b)("p",{parentName:"blockquote"},Object(a.b)("strong",{parentName:"p"},"Remark:")," The helper function above allows us to fine-tune the pre-trained ",Object(a.b)("strong",{parentName:"p"},"FastRCNNPredictor")," and ",Object(a.b)("strong",{parentName:"p"},"MaskRCNNPredictor")," with the desired number of classes, which are ",Object(a.b)("strong",{parentName:"p"},"'2'")," in our case i.e. for the 'BACKGROUND' and 'Student_ID' classes.\\\nThe function also sets the number of hidden layers of ",Object(a.b)("strong",{parentName:"p"},"MaskRCNNPredictor")," to ",Object(a.b)("strong",{parentName:"p"},"'256'")," but we can decide to tweak that for the best of our model performance.")),Object(a.b)("h3",{id:"122-specify-checkpoints-and-instantiate-model"},"1.2.2. Specify checkpoints and instantiate model"),Object(a.b)("p",null,"Looking forward to ",Object(a.b)("strong",{parentName:"p"},"resumable")," training and saving of our detection model, we shall now specify the checkpoints for the ",Object(a.b)("strong",{parentName:"p"},"state dictionaries")," of the model and its training optimizer."),Object(a.b)(c.a,{file:"detection_checkpoint",mdxType:"CodeBlock"}),Object(a.b)(d.a,{file:"detection_checkpoint_output",mdxType:"OutputBlock"}),Object(a.b)("h2",{id:"13-training-and-evaluation"},Object(a.b)("strong",{parentName:"h2"},"1.3. Training and Evaluation")),Object(a.b)("p",null,"Note that the files used for training and validation of detection module found ",Object(a.b)("inlineCode",{parentName:"p"},"./modules/detection/scripts")," folder were directly copied along with their dependencies from torchvision reference detection training scripts repository."),Object(a.b)("h3",{id:"131-specify-data-loaders"},"1.3.1. Specify data loaders"),Object(a.b)("p",null,"After initializing the various detection datasets, let us use them to specify data loaders which shall be used for training, validation, and testing."),Object(a.b)(c.a,{file:"detection_dataset_loaders",mdxType:"CodeBlock"}),Object(a.b)("h3",{id:"132-initialize-optimizer"},"1.3.2. Initialize optimizer"),Object(a.b)("p",null,"Let's initialize the optimizer for training the detection model, and get ready for training !"),Object(a.b)(c.a,{file:"detection_optimizer_init",mdxType:"CodeBlock"}),Object(a.b)("h3",{id:"133-define-training-function"},"1.3.3. Define training function"),Object(a.b)("p",null,"Now, let's write the function that will train and validate our model for us. Inside the training function, we shall add a few lines of code that will save our model checkpoints."),Object(a.b)(c.a,{file:"detection_model_train_function",mdxType:"CodeBlock"}),Object(a.b)("h3",{id:"134-train-detection-model"},"1.3.4 Train detection model"),Object(a.b)("p",null,"So let\u2019s train our detection model for 20 epochs saving it at the end of each epoch."),Object(a.b)(c.a,{file:"detection_model_train",mdxType:"CodeBlock"}),Object(a.b)(d.a,{file:"detection_model_train_output",mdxType:"OutputBlock"}),Object(a.b)("h3",{id:"135-resume-training-detection-model"},"1.3.5. Resume training detection model"),Object(a.b)("p",null,"At the end of every epoch, we had the checkpoints of the detection module updated. Now let's use these updated checkpoints to reload the detection model and resume its training up to ",Object(a.b)("strong",{parentName:"p"},"'30'")," epochs."),Object(a.b)("blockquote",null,Object(a.b)("p",{parentName:"blockquote"},Object(a.b)("strong",{parentName:"p"},"IMPORTANT !!!")," To reload the detection model and the detection optimizer from the checkpoint, simply re-run the code cell at Section 1.2.2. and Section 1.3.2 respectively. Just make sure ",Object(a.b)("inlineCode",{parentName:"p"},"load_detection_checkpoint")," is set to ",Object(a.b)("inlineCode",{parentName:"p"},"True"),". The resulting output shall be identical to the one below.")),Object(a.b)("p",null,"Reloading detection model from the checkpoint. (Section 1.2.2)"),Object(a.b)(d.a,{file:"detection_model_init_checkpoint_output",mdxType:"OutputBlock"}),Object(a.b)("p",null,"Reloading detection optimizer from the checkpoint (Section 1.3.2)"),Object(a.b)(d.a,{file:"detection_optimizer_init_checkpoint_output",mdxType:"OutputBlock"}),Object(a.b)("p",null,"Now let's resume training of our detection model."),Object(a.b)(c.a,{file:"detection_model_train_resume",mdxType:"CodeBlock"}),Object(a.b)(d.a,{file:"detection_model_train_resume_output",mdxType:"OutputBlock"}),Object(a.b)("p",null,"You notice that the training start from epoch 21 since the detection model has already been trained for 20 epochs."),Object(a.b)("h3",{id:"136-evaluate-the-detection-model"},"1.3.6. Evaluate the detection model"),Object(a.b)("p",null,"To conclude on the performance of your models, it is always of good practice to evaluate them on sample data."),Object(a.b)("p",null,"We shall evaluate the performance of the detection model on sample images from the testing dataset."),Object(a.b)("p",null,"Firstly, let's use our detection model to compute predictions for an input image from the test detection dataset."),Object(a.b)(c.a,{file:"detection_model_predict",mdxType:"CodeBlock"}),Object(a.b)("p",null,Object(a.b)("img",{alt:"img",src:o(333).default})),Object(a.b)("p",null,"Secondly, let's take a look at the raw outputs predicted by our detection model for the image above."),Object(a.b)(c.a,{file:"detection_model_predictions_raw",mdxType:"CodeBlock"}),Object(a.b)(d.a,{file:"detection_model_predictions_raw_output",mdxType:"OutputBlock"}),Object(a.b)("p",null,"Lastly, let's convert the raw predicted outputs into a human-understandable format for proper visualization."),Object(a.b)(c.a,{file:"detection_model_predictions_visualize",mdxType:"CodeBlock"}),Object(a.b)("p",null,Object(a.b)("img",{alt:"img",src:o(334).default})),Object(a.b)("h2",{id:"14-preparation-for-orientation-module"},Object(a.b)("strong",{parentName:"h2"},"1.4. Preparation for Orientation Module")),Object(a.b)("p",null,"At the beginning of this tutorial, we've mentioned that the goal of the detection module was to predict the position of documents of interest (which are student-ids in our case) with-in an image, but it does more than that. The segmentation mask computed for a student-id is used to perform image alignment on its corresponding student-id. The resulting aligned image is then fed as input to the orientation module."),Object(a.b)(c.a,{file:"detection_module_preparations",mdxType:"CodeBlock"}),Object(a.b)("p",null,Object(a.b)("img",{alt:"img",src:o(335).default})))}u.isMDXComponent=!0},87:function(e,t,o){"use strict";var i=o(0),n=o.n(i),a=o(337),c=o(336);class d extends n.a.Component{constructor(e){super(e),this.state={codeString:""}}static getDerivedStateFromProps(e,t){return e.id!==t.prevFile?{codeString:"",prevFile:e.file}:null}componentDidMount(){this._loadAsyncData(this.props.file)}componentDidUpdate(e,t){this.state.codeString||this._loadAsyncData(this.props.file)}render(){return n.a.createElement(a.a,{language:"python",style:c.a,customStyle:r,codeTagProps:{style:{color:"#e0e0e0"}}},this.state.codeString)}_loadAsyncData(e){fetch(`/docs/code-snippets/${e}.py`).then((e=>e.text())).then((e=>{this.setState({codeString:e})})).catch((e=>{console.log(e)}))}}t.a=d;const r={borderRadius:8,"font-size":"0.67em",overflow:"auto",maxHeight:"75vh"}},88:function(e,t,o){"use strict";var i=o(0),n=o.n(i);class a extends n.a.Component{constructor(e){super(e),this.state={outputString:""}}static getDerivedStateFromProps(e,t){return e.id!==t.prevFile?{outputString:"",prevFile:e.file}:null}componentDidMount(){this._loadAsyncData(this.props.file)}componentDidUpdate(e,t){this.state.outputString||this._loadAsyncData(this.props.file)}render(){return n.a.createElement("pre",{className:"output-block"},n.a.createElement("code",null,this.state.outputString))}_loadAsyncData(e){fetch(`/docs/code-snippets/${e}.txt`).then((e=>e.text())).then((e=>{this.setState({outputString:e})})).catch((e=>{console.log(e)}))}}t.a=a}}]);